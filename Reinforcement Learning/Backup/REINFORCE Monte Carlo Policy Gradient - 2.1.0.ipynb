{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# REINFORCE: Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T03:04:27.478911Z",
     "start_time": "2020-03-27T03:04:22.705747Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.layers import fully_connected as dense\n",
    "from tensorflow.compat.v1.layers import dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import laser_hockey_env as lh\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T03:04:36.594861Z",
     "start_time": "2020-03-27T03:04:36.502116Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = lh.LaserHockeyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1311..1643 -> 332-tiles track\n",
      "Track generation: 1231..1543 -> 312-tiles track\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-af228b41675a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys_to_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\utils\\play.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(env, transpose, fps, zoom, callback, keys_to_action)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeys_to_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpressed_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mprev_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from gym.utils import play\n",
    "\n",
    "\n",
    "key_mapping = {(97,108): 3, (100,108): 1, (108,119): 0, (108,115): 2}\n",
    "\n",
    "\n",
    "def callback(obs_t, obs_tp1, action, rew, done, info):\n",
    "    return [rew,]\n",
    "plotter = play.PlayPlot(callback, 30 * 1, [\"reward\"])\n",
    "\n",
    "\n",
    "play.play(env, callback=None, keys_to_action=key_mapping)\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T03:05:10.252959Z",
     "start_time": "2020-03-27T03:05:09.893093Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 23 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 42 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 34 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = {[1,2]:0}\n",
    "ord('w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Set up hyperparameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_episodes = 1000\n",
    "max_steps = 300\n",
    "\n",
    "# Hyperparameters\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.0001\n",
    "actor_bs = 8\n",
    "critic_bs = 8\n",
    "gamma = 0.98\n",
    "hidden_size = 16\n",
    "epsilon = 0.2\n",
    "\n",
    "# Amount of discrete actions per dimension for Box space\n",
    "k = 3\n",
    "\n",
    "# Not used hyperparameters\n",
    "lambda_ = 0.95\n",
    "\n",
    "# Choose REINFORCE or PPO loss function, PPO uses epsilon\n",
    "REINFORCE = False\n",
    "\n",
    "# Choose to linearly anneal the learning rate from 1 to 0 over the course of learning\n",
    "LINEAR = False\n",
    "\n",
    "# Optimize every episode or every T timesteps, depending on environment\n",
    "EPISODIC = True\n",
    "if EPISODIC: T = max_steps\n",
    "else: T = 128\n",
    "\n",
    "# Represent the discrete distribution as factorized across dimensions\n",
    "FACTORIZED = True\n",
    "\n",
    "# Choose to imitate basic agent in LaserHockey and if only for the winning cases\n",
    "IMITATION = True\n",
    "WINNER = True\n",
    "if env.unwrapped.spec is not None:\n",
    "    IMITATION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Set up Tensorboard\n",
    "To launch tensorboard : `tensorboard --logdir \"Google Drive/Reinforcement Learning/tensorboard/REINFORCE\"`\n",
    "\n",
    "`tensorboard --logdir \"Google Drive/Reinforcement Learning/tensorboard/REINFORCE/LunarLander-v2\"`\n",
    "\n",
    "`tensorboard --logdir \"tensorboard/REINFORCE\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Change parameters for different runs to have them seperate in Tensorboard\n",
    "writer = tf.summary.FileWriter('/tensorboard/REINFORCE/LaserHockey/'\n",
    "                               'E=' + str(max_steps) + ', T=' + str(T) +\n",
    "                               ', lr_a=' + str(actor_lr) + ', lr_c=' + str(critic_lr) +\n",
    "                               ', bs_a=' + str(actor_bs) + ', bs_c=' + str(critic_bs) +\n",
    "                               ', g=' + str(gamma) + ', hs=2x' + str(hidden_size) +\n",
    "                               ', e=0.2, vs basic 3x3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Show and compute specifications of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actionspace\n",
      "=========================\n",
      "Box(1,)\n",
      "1\n",
      "[2.]\n",
      "[-2.]\n",
      "=========================\n",
      "Observationspace\n",
      "=========================\n",
      "Box(3,)\n",
      "3\n",
      "[1. 1. 8.]\n",
      "[-1. -1. -8.]\n"
     ]
    }
   ],
   "source": [
    "print('Actionspace')\n",
    "print('=========================')\n",
    "print(env.action_space)\n",
    "if env.action_space.shape == ():\n",
    "    print(env.action_space.n)\n",
    "else:\n",
    "    print(env.action_space.shape[0])\n",
    "    print(env.action_space.high)\n",
    "    print(env.action_space.low)\n",
    "print('=========================')\n",
    "print('Observationspace')\n",
    "print('=========================')\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape[0])\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descretized actionspace and corresponding numbers\n",
      "=================================================\n",
      "[[ 1.   1.   1. ]\n",
      " [ 1.   1.   0.5]\n",
      " [ 1.   1.   0. ]\n",
      " [ 0.   1.   1. ]\n",
      " [ 0.   1.   0.5]\n",
      " [ 0.   1.   0. ]\n",
      " [-1.   1.   1. ]\n",
      " [-1.   1.   0.5]\n",
      " [-1.   1.   0. ]\n",
      " [ 1.   0.5  1. ]\n",
      " [ 1.   0.5  0.5]\n",
      " [ 1.   0.5  0. ]\n",
      " [ 0.   0.5  1. ]\n",
      " [ 0.   0.5  0.5]\n",
      " [ 0.   0.5  0. ]\n",
      " [-1.   0.5  1. ]\n",
      " [-1.   0.5  0.5]\n",
      " [-1.   0.5  0. ]\n",
      " [ 1.   0.   1. ]\n",
      " [ 1.   0.   0.5]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   0.   1. ]\n",
      " [ 0.   0.   0.5]\n",
      " [ 0.   0.   0. ]\n",
      " [-1.   0.   1. ]\n",
      " [-1.   0.   0.5]\n",
      " [-1.   0.   0. ]]\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n"
     ]
    }
   ],
   "source": [
    "# Compute dimensions for model's input/output sizes.\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "    # Checking if Descrete or Box action space\n",
    "if env.action_space.shape == ():\n",
    "    a_dim, action_dim = 1, env.action_space.n\n",
    "else:\n",
    "    # If environment is LaserHockey take only the first 3 action dimensions.\n",
    "    if env.unwrapped.spec is None: a_dim = 3\n",
    "    else: a_dim = env.action_space.shape[0]\n",
    "    if FACTORIZED: action_dim, policy_dim = k, a_dim\n",
    "    else: action_dim, policy_dim = k**a_dim, 1\n",
    "    \n",
    "    # Also compute descrete actionspace for the Box space, because the Descrete space doesn't need one.\n",
    "    actionspace = []\n",
    "    for d in range(a_dim):\n",
    "        actionspace.append(np.linspace(env.action_space.high[d], env.action_space.low[d], k))\n",
    "    actionspace = np.array(np.meshgrid(*actionspace)).reshape(a_dim, -1).T\n",
    "    actionspace_numbers = np.arange(k**a_dim)\n",
    "    actionspace_numbers = actionspace_numbers.reshape(np.full(policy_dim, action_dim))\n",
    "    \n",
    "    print('Descretized actionspace and corresponding numbers')\n",
    "    print('=================================================')\n",
    "    print(actionspace)\n",
    "    print(actionspace_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def descrete_action(action):\n",
    "    for i in range(3):\n",
    "        if action[i] > 0.5:\n",
    "            action[i] = 1\n",
    "        elif action[i] < -0.5:\n",
    "            action[i] = -1\n",
    "        else: action[i] = 0\n",
    "    return action\n",
    "\n",
    "def mean_list(lst):\n",
    "    if len(lst) != 0:\n",
    "        return sum(lst)/len(lst)\n",
    "    return 0\n",
    "\n",
    "def match_action_labels(action_labels):\n",
    "    for i in range(len(action_labels[0])):\n",
    "        for j in range(policy_dim):\n",
    "            action_labels[j][i][0] = i\n",
    "    return action_labels\n",
    "\n",
    "def action_label_to_action(action_label):\n",
    "    if env.action_space.shape == ():\n",
    "        return action_label[0]\n",
    "    return actionspace[actionspace_numbers.item(*action_label)]\n",
    "\n",
    "def action_to_action_label(action):\n",
    "    for i in range(k**a_dim):\n",
    "        if np.array_equal(actionspace[i], action):\n",
    "            return np.argwhere(actionspace_numbers == i).flatten()\n",
    "\n",
    "def discount_rewards(episode_r, discount):\n",
    "    value_f = 0\n",
    "    discounted_rewards = np.zeros_like(episode_r, dtype='float64')\n",
    "    for t in reversed(range(len(discounted_rewards))):\n",
    "        value_f = value_f * discount + episode_r[t]\n",
    "        discounted_rewards[t] = value_f\n",
    "    return discounted_rewards\n",
    "\n",
    "def td_error(episode_obs, episode_r):\n",
    "    td_errors = np.zeros(len(episode_r) - 1)\n",
    "    for t in range(len(td_errors)):\n",
    "        if t == (len(td_errors) - 1):\n",
    "            td_errors[t] = episode_r[t] + gamma * episode_r[-1] - critic.predict(episode_obs[t].reshape(1, -1))[0]\n",
    "        else:\n",
    "            td_errors[t] = episode_r[t] + gamma * critic.predict(episode_obs[t+1].reshape(1, -1))[0] - critic.predict(episode_obs[t].reshape(1, -1))[0]\n",
    "    return td_errors\n",
    "\n",
    "def subtract_baseline(episode_obs, value_f):\n",
    "    advantages = np.zeros_like(value_f)\n",
    "    for t in range(len(advantages)):\n",
    "#         advantages[t] = 1.0\n",
    "        advantages[t] = value_f[t] - critic.predict(episode_obs[t].reshape(1, -1))[0]\n",
    "    return advantages\n",
    "\n",
    "def clip_advantages(advantages):\n",
    "    clipped_advantages = np.zeros_like(advantages)\n",
    "    for i in range(len(clipped_advantages)):\n",
    "        if advantages[i] >= 0:\n",
    "            clipped_advantages[i] = (1 + epsilon) * advantages[i]\n",
    "        else: clipped_advantages[i] = (1 - epsilon) * advantages[i]\n",
    "    return clipped_advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Create Policy Gradient or PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ghost\\Anaconda3\\envs\\Test\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"inputs\"):    \n",
    "    # Placeholders for Tensorboard\n",
    "    reward_ = tf.placeholder(tf.float32, name='reward')\n",
    "#     actor_loss_ = tf.placeholder(tf.float32, name='actor_loss')\n",
    "    critic_loss_ = tf.placeholder(tf.float32, name='critic_loss')\n",
    "    \n",
    "    # Placeholders for training\n",
    "    input_ = tf.placeholder(tf.float32, [None, observation_dim], name='input')\n",
    "    action_label_ = tf.placeholder(tf.int32, [None, None, 2], name='action_label')\n",
    "    advantage_ = tf.placeholder(tf.float32, [None,], name='advantage')\n",
    "    clipped_advantage_ = tf.placeholder(tf.float32, [None,], name='clipped_advantage')\n",
    "    \n",
    "    with tf.name_scope('policy'):\n",
    "        relu_1, relu_2, action_distribution = {}, {}, {}\n",
    "        for i in range(policy_dim):\n",
    "            relu_1[i] = dense(inputs = input_, num_outputs = hidden_size, activation_fn = tf.nn.relu,\n",
    "                              weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "            relu_2[i] = dense(inputs = relu_1[i], num_outputs = hidden_size, activation_fn = tf.nn.relu,\n",
    "                              weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "            action_distribution[i] = dense(inputs = relu_2[i], num_outputs = action_dim, activation_fn = tf.nn.softmax,\n",
    "                                           weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "                \n",
    "    with tf.name_scope('policy_loss'):\n",
    "        joint_policy = tf.Variable(1.0)\n",
    "        for i in range(policy_dim):\n",
    "            joint_policy = joint_policy * tf.gather_nd(action_distribution[i], action_label_[i])\n",
    "            \n",
    "        if REINFORCE:\n",
    "            # Policy gradient objective function\n",
    "            policy_loss = tf.reduce_mean(tf.log(joint_policy + 1e-10) * advantage_)\n",
    "        else:\n",
    "            # PPO Clipped Surrogate Objective\n",
    "            old_policy = tf.stop_gradient(tf.identity(joint_policy))\n",
    "            policy_loss = tf.reduce_mean(tf.minimum(joint_policy / (old_policy + 1e-10) * advantage_, clipped_advantage_))\n",
    "    \n",
    "    with tf.name_scope('optimizers'):\n",
    "        optimizer = tf.train.AdamOptimizer(actor_lr).minimize(-policy_loss)\n",
    "        \n",
    "    \n",
    "tf.summary.scalar('Reward', reward_)\n",
    "# tf.summary.scalar('Policy_Loss', actor_loss_)\n",
    "tf.summary.scalar('Policy_Loss', policy_loss)\n",
    "tf.summary.scalar('Critic_Loss', critic_loss_)\n",
    "\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def actor_loss(old_prediction, advantage, clipped_advantage):\n",
    "    def loss(y_true, y_pred):\n",
    "#         new_prediction = tf.gather_nd(y_pred, tf.cast(y_true, tf.int64))\n",
    "        new_prediction = y_true * y_pred\n",
    "        if REINFORCE:\n",
    "            return -K.mean(K.log(new_prediction + 1e-10) * advantage)\n",
    "        r = new_prediction / (old_prediction + 1e-10)\n",
    "        return -K.mean(K.minimum(r * advantage, clipped_advantage))\n",
    "    return loss\n",
    "\n",
    "class decay_history(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.lr.append(linear_decay(episode))\n",
    "        print(' lr:', self.lr[-1])\n",
    "\n",
    "def linear_decay(episode):\n",
    "    decay = -(1 / max_episodes) * episode + 1\n",
    "    new_lr = critic_lr * decay\n",
    "    if LINEAR:\n",
    "        print(new_lr)\n",
    "        return new_lr\n",
    "    print(critic_lr)\n",
    "    return critic_lr\n",
    "\n",
    "decay_history = decay_history()\n",
    "new_lr = tf.keras.callbacks.LearningRateScheduler(linear_decay)\n",
    "# callbacks_list = [decay_history, new_lr]\n",
    "callbacks_list = [new_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "critic = Sequential([\n",
    "#     BatchNormalization(),\n",
    "    Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "    Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "    Dense(1, kernel_initializer='glorot_normal')])\n",
    "\n",
    "critic.compile(optimizer=Adam(lr=0.0), loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Didn't worked yet\n",
    "\n",
    "# def build_actor():\n",
    "#     state_input = Input(shape=(observation_dim,))\n",
    "#     old_prediction = Input(shape=(1,))\n",
    "#     advantage = Input(shape=(1,))\n",
    "#     clipped_advantage = Input(shape=(1,))\n",
    "\n",
    "#     x = Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu')(state_input)\n",
    "#     x = Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu')(x)\n",
    "#     action_distribution = Dense(action_dim, kernel_initializer='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input, old_prediction, advantage, clipped_advantage], outputs=[action_distribution])\n",
    "#     model.summary()\n",
    "#     model.compile(optimizer=Adam(lr=actor_lr), loss=[actor_loss(old_prediction, advantage, clipped_advantage)])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# actor = build_actor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "143/143 [==============================] - 0s 1ms/step - loss: 26.8317 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x00000168B1288048>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ghost\\Anaconda3\\envs\\Test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\Users\\ghost\\Anaconda3\\envs\\Test\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep:  143\n",
      "Episode:  1\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "148/148 [==============================] - 0s 108us/step - loss: 32.6948 - accuracy: 0.0000e+00\n",
      "Timestep:  148\n",
      "Episode:  2\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 121us/step - loss: 5.4628 - accuracy: 0.0933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  3\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 115us/step - loss: 43.8576 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  4\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "132/132 [==============================] - 0s 121us/step - loss: 38.7290 - accuracy: 0.0000e+00\n",
      "Timestep:  132\n",
      "Episode:  5\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "81/81 [==============================] - 0s 136us/step - loss: 35.0986 - accuracy: 0.0000e+00\n",
      "Timestep:  81\n",
      "Episode:  6\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "208/208 [==============================] - 0s 113us/step - loss: 32.6023 - accuracy: 0.0000e+00\n",
      "Timestep:  208\n",
      "Episode:  7\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 123us/step - loss: 7.4463 - accuracy: 0.1933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  8\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "255/255 [==============================] - 0s 102us/step - loss: 15.9118 - accuracy: 0.0000e+00\n",
      "Timestep:  255\n",
      "Episode:  9\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 2.9648 - accuracy: 0.2033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  10\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "76/76 [==============================] - 0s 145us/step - loss: 43.1783 - accuracy: 0.0000e+00\n",
      "Timestep:  76\n",
      "Episode:  11\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "75/75 [==============================] - 0s 147us/step - loss: 33.0691 - accuracy: 0.0000e+00\n",
      "Timestep:  75\n",
      "Episode:  12\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 4.2879 - accuracy: 0.1767\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  13\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 4.7453 - accuracy: 0.1300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  14\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "95/95 [==============================] - 0s 116us/step - loss: 23.0946 - accuracy: 0.0000e+00\n",
      "Timestep:  95\n",
      "Episode:  15\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "124/124 [==============================] - 0s 129us/step - loss: 24.6620 - accuracy: 0.0000e+00\n",
      "Timestep:  124\n",
      "Episode:  16\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "85/85 [==============================] - 0s 129us/step - loss: 28.0923 - accuracy: 0.0000e+00\n",
      "Timestep:  85\n",
      "Episode:  17\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 4.8079 - accuracy: 0.3800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  18\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "151/151 [==============================] - 0s 113us/step - loss: 12.0526 - accuracy: 0.0000e+00\n",
      "Timestep:  151\n",
      "Episode:  19\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "154/154 [==============================] - 0s 117us/step - loss: 22.0741 - accuracy: 0.0000e+00\n",
      "Timestep:  154\n",
      "Episode:  20\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "168/168 [==============================] - 0s 107us/step - loss: 18.4835 - accuracy: 0.0000e+00\n",
      "Timestep:  168\n",
      "Episode:  21\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 114us/step - loss: 5.2600 - accuracy: 0.2133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  22\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 4.5138 - accuracy: 0.2933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  23\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.6480 - accuracy: 0.3300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  24\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 2.1088 - accuracy: 0.2933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  25\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.7811 - accuracy: 0.3000\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  26\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "217/217 [==============================] - 0s 111us/step - loss: 14.8862 - accuracy: 0.0000e+00\n",
      "Timestep:  217\n",
      "Episode:  27\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.7515 - accuracy: 0.3867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  28\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 2.0332 - accuracy: 0.3733\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  29\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 0.5657 - accuracy: 0.4867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  30\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 122us/step - loss: 33.3191 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  31\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.1601 - accuracy: 0.3900\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  32\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 32.0814 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  33\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.3781 - accuracy: 0.4067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  34\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "67/67 [==============================] - 0s 134us/step - loss: 32.1096 - accuracy: 0.0000e+00\n",
      "Timestep:  67\n",
      "Episode:  35\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.3190 - accuracy: 0.4333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  36\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 130us/step - loss: 28.0964 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  37\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.8765 - accuracy: 0.4667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  38\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "79/79 [==============================] - 0s 127us/step - loss: 27.3294 - accuracy: 0.0000e+00\n",
      "Timestep:  79\n",
      "Episode:  39\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "69/69 [==============================] - 0s 130us/step - loss: 33.1038 - accuracy: 0.0000e+00\n",
      "Timestep:  69\n",
      "Episode:  40\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.8114 - accuracy: 0.5567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  41\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "173/173 [==============================] - 0s 116us/step - loss: 17.1411 - accuracy: 0.0000e+00\n",
      "Timestep:  173\n",
      "Episode:  42\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "60/60 [==============================] - 0s 133us/step - loss: 29.8555 - accuracy: 0.0000e+00\n",
      "Timestep:  60\n",
      "Episode:  43\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 125us/step - loss: 54.8325 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  44\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "146/146 [==============================] - 0s 116us/step - loss: 16.9697 - accuracy: 0.0000e+00\n",
      "Timestep:  146\n",
      "Episode:  45\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "66/66 [==============================] - 0s 303us/step - loss: 40.2952 - accuracy: 0.0000e+00\n",
      "Timestep:  66\n",
      "Episode:  46\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 104us/step - loss: 0.9087 - accuracy: 0.5133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  47\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "260/260 [==============================] - 0s 104us/step - loss: 8.5911 - accuracy: 0.0000e+00\n",
      "Timestep:  260\n",
      "Episode:  48\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.0232 - accuracy: 0.2967\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  49\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "95/95 [==============================] - 0s 137us/step - loss: 25.7750 - accuracy: 0.0000e+00\n",
      "Timestep:  95\n",
      "Episode:  50\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 108us/step - loss: 0.5332 - accuracy: 0.5300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  51\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "108/108 [==============================] - 0s 130us/step - loss: 27.0172 - accuracy: 0.0000e+00\n",
      "Timestep:  108\n",
      "Episode:  52\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 120us/step - loss: 0.6875 - accuracy: 0.3567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  53\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "92/92 [==============================] - 0s 163us/step - loss: 22.5451 - accuracy: 0.0000e+00\n",
      "Timestep:  92\n",
      "Episode:  54\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "192/192 [==============================] - 0s 130us/step - loss: 11.5797 - accuracy: 0.0000e+00\n",
      "Timestep:  192\n",
      "Episode:  55\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 135us/step - loss: 35.0539 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  56\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.7169 - accuracy: 0.5567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  57\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.2084 - accuracy: 0.2967\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  58\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "204/204 [==============================] - 0s 118us/step - loss: 9.2538 - accuracy: 0.0000e+00\n",
      "Timestep:  204\n",
      "Episode:  59\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "201/201 [==============================] - 0s 119us/step - loss: 12.0849 - accuracy: 0.0000e+00\n",
      "Timestep:  201\n",
      "Episode:  60\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "270/270 [==============================] - 0s 100us/step - loss: 9.3053 - accuracy: 0.0000e+00\n",
      "Timestep:  270\n",
      "Episode:  61\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "119/119 [==============================] - 0s 126us/step - loss: 18.1400 - accuracy: 0.0000e+00\n",
      "Timestep:  119\n",
      "Episode:  62\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.8695 - accuracy: 0.4100\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  63\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.7609 - accuracy: 0.4167\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  64\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "63/63 [==============================] - 0s 174us/step - loss: 29.9189 - accuracy: 0.0000e+00\n",
      "Timestep:  63\n",
      "Episode:  65\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.8220 - accuracy: 0.3300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  66\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "84/84 [==============================] - 0s 178us/step - loss: 28.9487 - accuracy: 0.0000e+00\n",
      "Timestep:  84\n",
      "Episode:  67\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.8478 - accuracy: 0.4367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  68\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.7478 - accuracy: 0.4333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  69\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "118/118 [==============================] - 0s 119us/step - loss: 15.3755 - accuracy: 0.0000e+00\n",
      "Timestep:  118\n",
      "Episode:  70\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 98us/step - loss: 1.8010 - accuracy: 0.2567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  71\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.2863 - accuracy: 0.2067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  72\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.7830 - accuracy: 0.4933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  73\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "147/147 [==============================] - 0s 116us/step - loss: 12.4170 - accuracy: 0.0000e+00\n",
      "Timestep:  147\n",
      "Episode:  74\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "162/162 [==============================] - 0s 111us/step - loss: 14.8085 - accuracy: 0.0000e+00\n",
      "Timestep:  162\n",
      "Episode:  75\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.5604 - accuracy: 0.5300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  76\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.6350 - accuracy: 0.4367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  77\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 115us/step - loss: 32.1845 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  78\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 156us/step - loss: 29.2943 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  79\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "65/65 [==============================] - 0s 138us/step - loss: 34.0199 - accuracy: 0.0000e+00\n",
      "Timestep:  65\n",
      "Episode:  80\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.4962 - accuracy: 0.4867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  81\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "135/135 [==============================] - 0s 118us/step - loss: 12.3013 - accuracy: 0.0000e+00\n",
      "Timestep:  135\n",
      "Episode:  82\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "119/119 [==============================] - 0s 118us/step - loss: 12.4981 - accuracy: 0.0000e+00\n",
      "Timestep:  119\n",
      "Episode:  83\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 29.0485 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  84\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "84/84 [==============================] - 0s 131us/step - loss: 29.9515 - accuracy: 0.0000e+00\n",
      "Timestep:  84\n",
      "Episode:  85\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.1376 - accuracy: 0.3333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  86\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.8968 - accuracy: 0.4067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  87\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "72/72 [==============================] - 0s 139us/step - loss: 23.6952 - accuracy: 0.0000e+00\n",
      "Timestep:  72\n",
      "Episode:  88\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.2657 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  89\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 135us/step - loss: 21.9366 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  90\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.0109 - accuracy: 0.3733\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  91\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "92/92 [==============================] - 0s 130us/step - loss: 16.9918 - accuracy: 0.0000e+00\n",
      "Timestep:  92\n",
      "Episode:  92\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.7765 - accuracy: 0.3300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  93\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 158us/step - loss: 27.8720 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  94\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "127/127 [==============================] - 0s 142us/step - loss: 19.7784 - accuracy: 0.0000e+00\n",
      "Timestep:  127\n",
      "Episode:  95\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 149us/step - loss: 20.6102 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  96\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.9274 - accuracy: 0.3767\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  97\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "204/204 [==============================] - 0s 108us/step - loss: 14.3336 - accuracy: 0.0000e+00\n",
      "Timestep:  204\n",
      "Episode:  98\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 135us/step - loss: 31.8499 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  99\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "146/146 [==============================] - 0s 123us/step - loss: 9.7433 - accuracy: 0.0000e+00\n",
      "Timestep:  146\n",
      "Episode:  100\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.1534 - accuracy: 0.5467\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  101\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 143us/step - loss: 27.4586 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  102\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "76/76 [==============================] - 0s 158us/step - loss: 27.6663 - accuracy: 0.0000e+00\n",
      "Timestep:  76\n",
      "Episode:  103\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "73/73 [==============================] - 0s 151us/step - loss: 20.7826 - accuracy: 0.0000e+00\n",
      "Timestep:  73\n",
      "Episode:  104\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 140us/step - loss: 3.1676 - accuracy: 0.2500\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  105\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 150us/step - loss: 0.6177 - accuracy: 0.4400\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  106\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "105/105 [==============================] - 0s 124us/step - loss: 13.1207 - accuracy: 0.0000e+00\n",
      "Timestep:  105\n",
      "Episode:  107\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.6436 - accuracy: 0.3567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  108\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "58/58 [==============================] - 0s 138us/step - loss: 32.6186 - accuracy: 0.0000e+00\n",
      "Timestep:  58\n",
      "Episode:  109\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "150/150 [==============================] - 0s 133us/step - loss: 10.8394 - accuracy: 0.0000e+00\n",
      "Timestep:  150\n",
      "Episode:  110\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "255/255 [==============================] - 0s 102us/step - loss: 7.7231 - accuracy: 0.0000e+00\n",
      "Timestep:  255\n",
      "Episode:  111\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "84/84 [==============================] - 0s 155us/step - loss: 29.5141 - accuracy: 0.0000e+00\n",
      "Timestep:  84\n",
      "Episode:  112\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.5698 - accuracy: 0.3033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  113\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "69/69 [==============================] - 0s 130us/step - loss: 23.2307 - accuracy: 0.0000e+00\n",
      "Timestep:  69\n",
      "Episode:  114\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 2.0183 - accuracy: 0.3633\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  115\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.6459 - accuracy: 0.5333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  116\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "83/83 [==============================] - 0s 144us/step - loss: 26.1657 - accuracy: 0.0000e+00\n",
      "Timestep:  83\n",
      "Episode:  117\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "69/69 [==============================] - 0s 159us/step - loss: 24.9027 - accuracy: 0.0000e+00\n",
      "Timestep:  69\n",
      "Episode:  118\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "171/171 [==============================] - 0s 111us/step - loss: 16.2363 - accuracy: 0.0000e+00\n",
      "Timestep:  171\n",
      "Episode:  119\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 158us/step - loss: 27.3752 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  120\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 128us/step - loss: 24.8335 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  121\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.4038 - accuracy: 0.3133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  122\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "80/80 [==============================] - 0s 125us/step - loss: 27.8856 - accuracy: 0.0000e+00\n",
      "Timestep:  80\n",
      "Episode:  123\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "107/107 [==============================] - 0s 112us/step - loss: 19.7418 - accuracy: 0.0000e+00\n",
      "Timestep:  107\n",
      "Episode:  124\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 36.8719 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  125\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "190/190 [==============================] - 0s 100us/step - loss: 8.9655 - accuracy: 0.0000e+00\n",
      "Timestep:  190\n",
      "Episode:  126\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 143us/step - loss: 30.3089 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  127\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.5394 - accuracy: 0.3867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  128\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 3.7680 - accuracy: 0.1467\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  129\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 2.4293 - accuracy: 0.2533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  130\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "85/85 [==============================] - 0s 118us/step - loss: 24.3341 - accuracy: 0.0000e+00\n",
      "Timestep:  85\n",
      "Episode:  131\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "65/65 [==============================] - 0s 138us/step - loss: 34.2770 - accuracy: 0.0000e+00\n",
      "Timestep:  65\n",
      "Episode:  132\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "70/70 [==============================] - 0s 129us/step - loss: 26.1267 - accuracy: 0.0000e+00\n",
      "Timestep:  70\n",
      "Episode:  133\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "154/154 [==============================] - 0s 110us/step - loss: 13.9501 - accuracy: 0.0000e+00\n",
      "Timestep:  154\n",
      "Episode:  134\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "273/273 [==============================] - 0s 113us/step - loss: 9.8376 - accuracy: 0.0000e+00\n",
      "Timestep:  273\n",
      "Episode:  135\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "132/132 [==============================] - 0s 121us/step - loss: 16.6306 - accuracy: 0.0000e+00\n",
      "Timestep:  132\n",
      "Episode:  136\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "97/97 [==============================] - 0s 134us/step - loss: 20.2042 - accuracy: 0.0000e+00\n",
      "Timestep:  97\n",
      "Episode:  137\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.9076 - accuracy: 0.3833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  138\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.9818 - accuracy: 0.2967\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  139\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.5701 - accuracy: 0.4367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  140\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "224/224 [==============================] - 0s 100us/step - loss: 8.0591 - accuracy: 0.0000e+00\n",
      "Timestep:  224\n",
      "Episode:  141\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 130us/step - loss: 26.9489 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  142\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "156/156 [==============================] - 0s 109us/step - loss: 14.7842 - accuracy: 0.0000e+00\n",
      "Timestep:  156\n",
      "Episode:  143\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "72/72 [==============================] - 0s 139us/step - loss: 30.5552 - accuracy: 0.0000e+00\n",
      "Timestep:  72\n",
      "Episode:  144\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "75/75 [==============================] - 0s 133us/step - loss: 23.9326 - accuracy: 0.0000e+00\n",
      "Timestep:  75\n",
      "Episode:  145\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 130us/step - loss: 24.7654 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  146\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "91/91 [==============================] - 0s 132us/step - loss: 21.8222 - accuracy: 0.0000e+00\n",
      "Timestep:  91\n",
      "Episode:  147\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "192/192 [==============================] - 0s 109us/step - loss: 9.4740 - accuracy: 0.0000e+00\n",
      "Timestep:  192\n",
      "Episode:  148\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "144/144 [==============================] - 0s 132us/step - loss: 12.6616 - accuracy: 0.0000e+00\n",
      "Timestep:  144\n",
      "Episode:  149\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 3.2609 - accuracy: 0.0933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  150\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.6161 - accuracy: 0.4533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  151\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.6548 - accuracy: 0.1867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  152\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 0.2951 - accuracy: 0.6333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  153\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "80/80 [==============================] - 0s 125us/step - loss: 19.8483 - accuracy: 0.0000e+00\n",
      "Timestep:  80\n",
      "Episode:  154\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "209/209 [==============================] - 0s 115us/step - loss: 9.3563 - accuracy: 0.0000e+00\n",
      "Timestep:  209\n",
      "Episode:  155\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "273/273 [==============================] - 0s 107us/step - loss: 9.1864 - accuracy: 0.0000e+00\n",
      "Timestep:  273\n",
      "Episode:  156\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 2.2692 - accuracy: 0.2033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  157\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "190/190 [==============================] - 0s 116us/step - loss: 11.6361 - accuracy: 0.0000e+00\n",
      "Timestep:  190\n",
      "Episode:  158\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.9265 - accuracy: 0.3533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  159\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "238/238 [==============================] - 0s 109us/step - loss: 8.8769 - accuracy: 0.0000e+00\n",
      "Timestep:  238\n",
      "Episode:  160\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "84/84 [==============================] - 0s 143us/step - loss: 24.7053 - accuracy: 0.0000e+00\n",
      "Timestep:  84\n",
      "Episode:  161\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "150/150 [==============================] - 0s 120us/step - loss: 12.2026 - accuracy: 0.0000e+00\n",
      "Timestep:  150\n",
      "Episode:  162\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 105us/step - loss: 1.4614 - accuracy: 0.3600\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  163\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "169/169 [==============================] - 0s 124us/step - loss: 9.5788 - accuracy: 0.0000e+00\n",
      "Timestep:  169\n",
      "Episode:  164\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.3555 - accuracy: 0.6300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  165\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.5645 - accuracy: 0.4933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  166\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "276/276 [==============================] - 0s 105us/step - loss: 6.5529 - accuracy: 0.0000e+00\n",
      "Timestep:  276\n",
      "Episode:  167\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "58/58 [==============================] - 0s 138us/step - loss: 24.1004 - accuracy: 0.0000e+00\n",
      "Timestep:  58\n",
      "Episode:  168\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.7519 - accuracy: 0.3933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  169\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.0869 - accuracy: 0.3133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  170\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "110/110 [==============================] - 0s 127us/step - loss: 15.6817 - accuracy: 0.0000e+00\n",
      "Timestep:  110\n",
      "Episode:  171\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "97/97 [==============================] - 0s 113us/step - loss: 17.8978 - accuracy: 0.0000e+00\n",
      "Timestep:  97\n",
      "Episode:  172\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "278/278 [==============================] - 0s 104us/step - loss: 6.8761 - accuracy: 0.0000e+00\n",
      "Timestep:  278\n",
      "Episode:  173\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 118us/step - loss: 0.9983 - accuracy: 0.5000\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  174\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "293/293 [==============================] - 0s 109us/step - loss: 5.5399 - accuracy: 0.0000e+00\n",
      "Timestep:  293\n",
      "Episode:  175\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.9387 - accuracy: 0.2667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  176\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "61/61 [==============================] - 0s 147us/step - loss: 34.3755 - accuracy: 0.0000e+00\n",
      "Timestep:  61\n",
      "Episode:  177\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 128us/step - loss: 29.2199 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  178\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.4461 - accuracy: 0.3733\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  179\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.4575 - accuracy: 0.6067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  180\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "174/174 [==============================] - 0s 109us/step - loss: 9.4936 - accuracy: 0.0000e+00\n",
      "Timestep:  174\n",
      "Episode:  181\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.8946 - accuracy: 0.2567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  182\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 122us/step - loss: 31.0096 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  183\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.8673 - accuracy: 0.4033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  184\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.9117 - accuracy: 0.4200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  185\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.6146 - accuracy: 0.4833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  186\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.4347 - accuracy: 0.6067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  187\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 115us/step - loss: 19.8899 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  188\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "68/68 [==============================] - 0s 132us/step - loss: 32.1261 - accuracy: 0.0000e+00\n",
      "Timestep:  68\n",
      "Episode:  189\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "258/258 [==============================] - 0s 108us/step - loss: 6.0640 - accuracy: 0.0000e+00\n",
      "Timestep:  258\n",
      "Episode:  190\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.5194 - accuracy: 0.4367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  191\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "125/125 [==============================] - 0s 120us/step - loss: 18.7420 - accuracy: 0.0000e+00\n",
      "Timestep:  125\n",
      "Episode:  192\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "70/70 [==============================] - 0s 143us/step - loss: 27.6189 - accuracy: 0.0000e+00\n",
      "Timestep:  70\n",
      "Episode:  193\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.0269 - accuracy: 0.3133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  194\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "87/87 [==============================] - 0s 126us/step - loss: 25.0728 - accuracy: 0.0000e+00\n",
      "Timestep:  87\n",
      "Episode:  195\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "231/231 [==============================] - 0s 117us/step - loss: 9.6932 - accuracy: 0.0000e+00\n",
      "Timestep:  231\n",
      "Episode:  196\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 2.0598 - accuracy: 0.2867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  197\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.7331 - accuracy: 0.4933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  198\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "92/92 [==============================] - 0s 130us/step - loss: 21.9002 - accuracy: 0.0000e+00\n",
      "Timestep:  92\n",
      "Episode:  199\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.1483 - accuracy: 0.3667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  200\n",
      "=====================================\n",
      "Model saved\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "122/122 [==============================] - 0s 123us/step - loss: 17.6401 - accuracy: 0.0000e+00\n",
      "Timestep:  122\n",
      "Episode:  201\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "72/72 [==============================] - 0s 125us/step - loss: 26.1374 - accuracy: 0.0000e+00\n",
      "Timestep:  72\n",
      "Episode:  202\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "59/59 [==============================] - 0s 152us/step - loss: 29.8315 - accuracy: 0.0000e+00\n",
      "Timestep:  59\n",
      "Episode:  203\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "245/245 [==============================] - 0s 126us/step - loss: 9.0348 - accuracy: 0.0000e+00\n",
      "Timestep:  245\n",
      "Episode:  204\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "200/200 [==============================] - 0s 120us/step - loss: 10.7574 - accuracy: 0.0000e+00\n",
      "Timestep:  200\n",
      "Episode:  205\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 210us/step - loss: 29.3000 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  206\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 150us/step - loss: 1.6122 - accuracy: 0.4300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  207\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "59/59 [==============================] - 0s 135us/step - loss: 27.6650 - accuracy: 0.0000e+00\n",
      "Timestep:  59\n",
      "Episode:  208\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 109us/step - loss: 0.6555 - accuracy: 0.4700\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  209\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 2.0012 - accuracy: 0.3567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  210\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.4949 - accuracy: 0.5267\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  211\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.6418 - accuracy: 0.4900\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  212\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.3195 - accuracy: 0.5633\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  213\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "92/92 [==============================] - 0s 130us/step - loss: 17.1826 - accuracy: 0.0000e+00\n",
      "Timestep:  92\n",
      "Episode:  214\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.6569 - accuracy: 0.5667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  215\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "58/58 [==============================] - 0s 172us/step - loss: 25.3042 - accuracy: 0.0000e+00\n",
      "Timestep:  58\n",
      "Episode:  216\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "165/165 [==============================] - 0s 115us/step - loss: 16.2647 - accuracy: 0.0000e+00\n",
      "Timestep:  165\n",
      "Episode:  217\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "196/196 [==============================] - 0s 107us/step - loss: 13.1312 - accuracy: 0.0000e+00\n",
      "Timestep:  196\n",
      "Episode:  218\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "60/60 [==============================] - 0s 150us/step - loss: 29.1824 - accuracy: 0.0000e+00\n",
      "Timestep:  60\n",
      "Episode:  219\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.9154 - accuracy: 0.5667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  220\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "82/82 [==============================] - 0s 134us/step - loss: 24.6544 - accuracy: 0.0000e+00\n",
      "Timestep:  82\n",
      "Episode:  221\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "197/197 [==============================] - 0s 117us/step - loss: 9.9011 - accuracy: 0.0000e+00\n",
      "Timestep:  197\n",
      "Episode:  222\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 143us/step - loss: 29.0441 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  223\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 1.0998 - accuracy: 0.2933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  224\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "90/90 [==============================] - 0s 133us/step - loss: 18.4615 - accuracy: 0.0000e+00\n",
      "Timestep:  90\n",
      "Episode:  225\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "65/65 [==============================] - 0s 154us/step - loss: 28.6445 - accuracy: 0.0000e+00\n",
      "Timestep:  65\n",
      "Episode:  226\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "163/163 [==============================] - 0s 123us/step - loss: 13.3914 - accuracy: 0.0000e+00\n",
      "Timestep:  163\n",
      "Episode:  227\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.5820 - accuracy: 0.3933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  228\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "101/101 [==============================] - 0s 119us/step - loss: 19.6222 - accuracy: 0.0000e+00\n",
      "Timestep:  101\n",
      "Episode:  229\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "104/104 [==============================] - 0s 125us/step - loss: 17.6901 - accuracy: 0.0000e+00\n",
      "Timestep:  104\n",
      "Episode:  230\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 158us/step - loss: 27.9065 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  231\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "236/236 [==============================] - 0s 106us/step - loss: 10.0791 - accuracy: 0.0000e+00\n",
      "Timestep:  236\n",
      "Episode:  232\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "63/63 [==============================] - 0s 127us/step - loss: 29.3768 - accuracy: 0.0000e+00\n",
      "Timestep:  63\n",
      "Episode:  233\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 104us/step - loss: 0.4932 - accuracy: 0.4733\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  234\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "81/81 [==============================] - 0s 123us/step - loss: 19.5465 - accuracy: 0.0000e+00\n",
      "Timestep:  81\n",
      "Episode:  235\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.4699 - accuracy: 0.6200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  236\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "61/61 [==============================] - 0s 131us/step - loss: 26.5051 - accuracy: 0.0000e+00\n",
      "Timestep:  61\n",
      "Episode:  237\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "109/109 [==============================] - 0s 110us/step - loss: 20.7537 - accuracy: 0.0000e+00\n",
      "Timestep:  109\n",
      "Episode:  238\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "104/104 [==============================] - 0s 115us/step - loss: 15.3371 - accuracy: 0.0000e+00\n",
      "Timestep:  104\n",
      "Episode:  239\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.6790 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  240\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "88/88 [==============================] - 0s 114us/step - loss: 19.0614 - accuracy: 0.0000e+00\n",
      "Timestep:  88\n",
      "Episode:  241\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "171/171 [==============================] - 0s 105us/step - loss: 10.8983 - accuracy: 0.0000e+00\n",
      "Timestep:  171\n",
      "Episode:  242\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 106us/step - loss: 0.9184 - accuracy: 0.2867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  243\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "90/90 [==============================] - 0s 133us/step - loss: 23.6588 - accuracy: 0.0000e+00\n",
      "Timestep:  90\n",
      "Episode:  244\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.2357 - accuracy: 0.5933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  245\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "109/109 [==============================] - 0s 129us/step - loss: 17.7367 - accuracy: 0.0000e+00\n",
      "Timestep:  109\n",
      "Episode:  246\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.7727 - accuracy: 0.5867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  247\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 130us/step - loss: 27.3689 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  248\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 1.7884 - accuracy: 0.2667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  249\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.9638 - accuracy: 0.4700\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  250\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.7443 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  251\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "116/116 [==============================] - 0s 112us/step - loss: 18.4569 - accuracy: 0.0000e+00\n",
      "Timestep:  116\n",
      "Episode:  252\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 158us/step - loss: 31.6145 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  253\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 123us/step - loss: 0.8309 - accuracy: 0.5033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  254\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "254/254 [==============================] - 0s 102us/step - loss: 8.4972 - accuracy: 0.0000e+00\n",
      "Timestep:  254\n",
      "Episode:  255\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.5427 - accuracy: 0.2367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  256\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 23.6095 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  257\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "67/67 [==============================] - 0s 134us/step - loss: 28.6182 - accuracy: 0.0000e+00\n",
      "Timestep:  67\n",
      "Episode:  258\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.7612 - accuracy: 0.3300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  259\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.3279 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  260\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "60/60 [==============================] - 0s 150us/step - loss: 27.9063 - accuracy: 0.0000e+00\n",
      "Timestep:  60\n",
      "Episode:  261\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.2788 - accuracy: 0.6800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  262\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "68/68 [==============================] - 0s 132us/step - loss: 22.8081 - accuracy: 0.0000e+00\n",
      "Timestep:  68\n",
      "Episode:  263\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "59/59 [==============================] - 0s 152us/step - loss: 33.3268 - accuracy: 0.0000e+00\n",
      "Timestep:  59\n",
      "Episode:  264\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.8816 - accuracy: 0.3067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  265\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "127/127 [==============================] - 0s 118us/step - loss: 14.5005 - accuracy: 0.0000e+00\n",
      "Timestep:  127\n",
      "Episode:  266\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "121/121 [==============================] - 0s 140us/step - loss: 14.1253 - accuracy: 0.0000e+00\n",
      "Timestep:  121\n",
      "Episode:  267\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "256/256 [==============================] - 0s 105us/step - loss: 5.6634 - accuracy: 0.0000e+00\n",
      "Timestep:  256\n",
      "Episode:  268\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "142/142 [==============================] - 0s 113us/step - loss: 13.1283 - accuracy: 0.0000e+00\n",
      "Timestep:  142\n",
      "Episode:  269\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.6434 - accuracy: 0.3933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  270\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.5551 - accuracy: 0.4667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  271\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.0126 - accuracy: 0.3267\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  272\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "66/66 [==============================] - 0s 136us/step - loss: 20.5489 - accuracy: 0.0000e+00\n",
      "Timestep:  66\n",
      "Episode:  273\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 1.0504 - accuracy: 0.4900\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  274\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.1706 - accuracy: 0.2500\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  275\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "81/81 [==============================] - 0s 123us/step - loss: 25.8282 - accuracy: 0.0000e+00\n",
      "Timestep:  81\n",
      "Episode:  276\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "73/73 [==============================] - 0s 137us/step - loss: 22.5397 - accuracy: 0.0000e+00\n",
      "Timestep:  73\n",
      "Episode:  277\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "66/66 [==============================] - 0s 136us/step - loss: 31.0265 - accuracy: 0.0000e+00\n",
      "Timestep:  66\n",
      "Episode:  278\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "81/81 [==============================] - 0s 123us/step - loss: 20.7563 - accuracy: 0.0000e+00\n",
      "Timestep:  81\n",
      "Episode:  279\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 158us/step - loss: 32.7723 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  280\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.4400 - accuracy: 0.5500\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  281\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "157/157 [==============================] - 0s 108us/step - loss: 13.1296 - accuracy: 0.0000e+00\n",
      "Timestep:  157\n",
      "Episode:  282\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 0.3917 - accuracy: 0.5200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  283\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 163us/step - loss: 2.1893 - accuracy: 0.3033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  284\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "219/219 [==============================] - 0s 105us/step - loss: 9.5091 - accuracy: 0.0000e+00\n",
      "Timestep:  219\n",
      "Episode:  285\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "98/98 [==============================] - 0s 143us/step - loss: 19.5249 - accuracy: 0.0000e+00\n",
      "Timestep:  98\n",
      "Episode:  286\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.9565 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  287\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "103/103 [==============================] - 0s 146us/step - loss: 14.6566 - accuracy: 0.0000e+00\n",
      "Timestep:  103\n",
      "Episode:  288\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.8029 - accuracy: 0.3033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  289\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.6803 - accuracy: 0.2967\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  290\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.3867 - accuracy: 0.2800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  291\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.1968 - accuracy: 0.5600\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  292\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 122us/step - loss: 23.4471 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  293\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.4458 - accuracy: 0.4433\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  294\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 2.1777 - accuracy: 0.2567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  295\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "74/74 [==============================] - 0s 135us/step - loss: 21.0013 - accuracy: 0.0000e+00\n",
      "Timestep:  74\n",
      "Episode:  296\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 101us/step - loss: 0.9539 - accuracy: 0.3400\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  297\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.4653 - accuracy: 0.4667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  298\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "80/80 [==============================] - 0s 112us/step - loss: 22.9605 - accuracy: 0.0000e+00\n",
      "Timestep:  80\n",
      "Episode:  299\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "166/166 [==============================] - 0s 105us/step - loss: 10.8729 - accuracy: 0.0000e+00\n",
      "Timestep:  166\n",
      "Episode:  300\n",
      "=====================================\n",
      "Model saved\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "73/73 [==============================] - 0s 151us/step - loss: 25.6775 - accuracy: 0.0000e+00\n",
      "Timestep:  73\n",
      "Episode:  301\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "63/63 [==============================] - 0s 127us/step - loss: 24.6474 - accuracy: 0.0000e+00\n",
      "Timestep:  63\n",
      "Episode:  302\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "113/113 [==============================] - 0s 115us/step - loss: 14.0416 - accuracy: 0.0000e+00\n",
      "Timestep:  113\n",
      "Episode:  303\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.5486 - accuracy: 0.2533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  304\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 1.1751 - accuracy: 0.4533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  305\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 101us/step - loss: 0.8299 - accuracy: 0.5067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  306\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.3004 - accuracy: 0.4567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  307\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 130us/step - loss: 0.5855 - accuracy: 0.5300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  308\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.4053 - accuracy: 0.5067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  309\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "75/75 [==============================] - 0s 133us/step - loss: 20.8494 - accuracy: 0.0000e+00\n",
      "Timestep:  75\n",
      "Episode:  310\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "72/72 [==============================] - 0s 111us/step - loss: 26.0429 - accuracy: 0.0000e+00\n",
      "Timestep:  72\n",
      "Episode:  311\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "86/86 [==============================] - 0s 128us/step - loss: 22.9836 - accuracy: 0.0000e+00\n",
      "Timestep:  86\n",
      "Episode:  312\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 101us/step - loss: 0.5637 - accuracy: 0.3867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  313\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 1.6190 - accuracy: 0.2567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  314\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "172/172 [==============================] - 0s 110us/step - loss: 14.9364 - accuracy: 0.0000e+00\n",
      "Timestep:  172\n",
      "Episode:  315\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 141us/step - loss: 24.1682 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  316\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.4929 - accuracy: 0.5067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  317\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.8507 - accuracy: 0.5000\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  318\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "97/97 [==============================] - 0s 124us/step - loss: 22.7763 - accuracy: 0.0000e+00\n",
      "Timestep:  97\n",
      "Episode:  319\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 0.4151 - accuracy: 0.5767\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  320\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "164/164 [==============================] - 0s 104us/step - loss: 12.6110 - accuracy: 0.0000e+00\n",
      "Timestep:  164\n",
      "Episode:  321\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 133us/step - loss: 0.6218 - accuracy: 0.4167\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  322\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 130us/step - loss: 0.7187 - accuracy: 0.3167\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  323\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 0.5019 - accuracy: 0.4700\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  324\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 1.1756 - accuracy: 0.3500\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  325\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 115us/step - loss: 23.8115 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  326\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "159/159 [==============================] - 0s 101us/step - loss: 14.3472 - accuracy: 0.0000e+00\n",
      "Timestep:  159\n",
      "Episode:  327\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "62/62 [==============================] - 0s 129us/step - loss: 26.9580 - accuracy: 0.0000e+00\n",
      "Timestep:  62\n",
      "Episode:  328\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "83/83 [==============================] - 0s 120us/step - loss: 20.9773 - accuracy: 0.0000e+00\n",
      "Timestep:  83\n",
      "Episode:  329\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 0.5438 - accuracy: 0.6833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  330\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "229/229 [==============================] - 0s 101us/step - loss: 9.8908 - accuracy: 0.0000e+00\n",
      "Timestep:  229\n",
      "Episode:  331\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "116/116 [==============================] - 0s 112us/step - loss: 15.4822 - accuracy: 0.0000e+00\n",
      "Timestep:  116\n",
      "Episode:  332\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 98us/step - loss: 1.0683 - accuracy: 0.3667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  333\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 125us/step - loss: 26.2387 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  334\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.7873 - accuracy: 0.4867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  335\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 128us/step - loss: 18.8160 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  336\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "200/200 [==============================] - 0s 110us/step - loss: 7.9648 - accuracy: 0.0000e+00\n",
      "Timestep:  200\n",
      "Episode:  337\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "204/204 [==============================] - 0s 108us/step - loss: 8.1756 - accuracy: 0.0000e+00\n",
      "Timestep:  204\n",
      "Episode:  338\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 36.5121 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  339\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "95/95 [==============================] - 0s 126us/step - loss: 12.2405 - accuracy: 0.0000e+00\n",
      "Timestep:  95\n",
      "Episode:  340\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "69/69 [==============================] - 0s 116us/step - loss: 17.1927 - accuracy: 0.0000e+00\n",
      "Timestep:  69\n",
      "Episode:  341\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "162/162 [==============================] - 0s 111us/step - loss: 6.5476 - accuracy: 0.0000e+00\n",
      "Timestep:  162\n",
      "Episode:  342\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "89/89 [==============================] - 0s 141us/step - loss: 26.1008 - accuracy: 0.0000e+00\n",
      "Timestep:  89\n",
      "Episode:  343\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "85/85 [==============================] - 0s 141us/step - loss: 26.4889 - accuracy: 0.0000e+00\n",
      "Timestep:  85\n",
      "Episode:  344\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 104us/step - loss: 0.3209 - accuracy: 0.7000\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  345\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 4.1724 - accuracy: 0.1633\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  346\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "69/69 [==============================] - 0s 130us/step - loss: 28.0151 - accuracy: 0.0000e+00\n",
      "Timestep:  69\n",
      "Episode:  347\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "59/59 [==============================] - 0s 135us/step - loss: 18.8617 - accuracy: 0.0000e+00\n",
      "Timestep:  59\n",
      "Episode:  348\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 158us/step - loss: 30.1626 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  349\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "261/261 [==============================] - 0s 100us/step - loss: 7.4444 - accuracy: 0.0000e+00\n",
      "Timestep:  261\n",
      "Episode:  350\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 102us/step - loss: 0.2435 - accuracy: 0.7100\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  351\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 123us/step - loss: 18.8998 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  352\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 125us/step - loss: 30.9672 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  353\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "184/184 [==============================] - 0s 109us/step - loss: 8.7502 - accuracy: 0.0000e+00\n",
      "Timestep:  184\n",
      "Episode:  354\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 95us/step - loss: 2.2084 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  355\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "81/81 [==============================] - 0s 123us/step - loss: 12.4780 - accuracy: 0.0000e+00\n",
      "Timestep:  81\n",
      "Episode:  356\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.1588 - accuracy: 0.3500\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  357\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 104us/step - loss: 2.5660 - accuracy: 0.3233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  358\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "97/97 [==============================] - 0s 124us/step - loss: 11.4022 - accuracy: 0.0000e+00\n",
      "Timestep:  97\n",
      "Episode:  359\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "252/252 [==============================] - 0s 104us/step - loss: 9.1609 - accuracy: 0.0000e+00\n",
      "Timestep:  252\n",
      "Episode:  360\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 133us/step - loss: 0.9415 - accuracy: 0.5467\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  361\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 101us/step - loss: 2.2250 - accuracy: 0.4033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  362\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.8532 - accuracy: 0.4000\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  363\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "285/285 [==============================] - 0s 109us/step - loss: 8.7551 - accuracy: 0.0000e+00\n",
      "Timestep:  285\n",
      "Episode:  364\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 105us/step - loss: 1.4873 - accuracy: 0.2367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  365\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 1.7122 - accuracy: 0.2667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  366\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 105us/step - loss: 0.5051 - accuracy: 0.4800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  367\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 1.3476 - accuracy: 0.2933\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  368\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.6431 - accuracy: 0.4800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  369\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "174/174 [==============================] - 0s 115us/step - loss: 13.1221 - accuracy: 0.0000e+00\n",
      "Timestep:  174\n",
      "Episode:  370\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "96/96 [==============================] - 0s 135us/step - loss: 21.3052 - accuracy: 0.0000e+00\n",
      "Timestep:  96\n",
      "Episode:  371\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "142/142 [==============================] - 0s 113us/step - loss: 14.5827 - accuracy: 0.0000e+00\n",
      "Timestep:  142\n",
      "Episode:  372\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 30.0723 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  373\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 0.7084 - accuracy: 0.5200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  374\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 117us/step - loss: 0.6205 - accuracy: 0.4533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  375\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "145/145 [==============================] - 0s 124us/step - loss: 13.7356 - accuracy: 0.0000e+00\n",
      "Timestep:  145\n",
      "Episode:  376\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 130us/step - loss: 0.3528 - accuracy: 0.6200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  377\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.3342 - accuracy: 0.4467\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  378\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "186/186 [==============================] - 0s 124us/step - loss: 12.4434 - accuracy: 0.0000e+00\n",
      "Timestep:  186\n",
      "Episode:  379\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "83/83 [==============================] - 0s 120us/step - loss: 20.8842 - accuracy: 0.0000e+00\n",
      "Timestep:  83\n",
      "Episode:  380\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "99/99 [==============================] - 0s 121us/step - loss: 18.7806 - accuracy: 0.0000e+00\n",
      "Timestep:  99\n",
      "Episode:  381\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "130/130 [==============================] - 0s 115us/step - loss: 18.8371 - accuracy: 0.0000e+00\n",
      "Timestep:  130\n",
      "Episode:  382\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "103/103 [==============================] - 0s 126us/step - loss: 14.8461 - accuracy: 0.0000e+00\n",
      "Timestep:  103\n",
      "Episode:  383\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 141us/step - loss: 22.3747 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  384\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.8427 - accuracy: 0.3333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  385\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "67/67 [==============================] - 0s 149us/step - loss: 28.2437 - accuracy: 0.0000e+00\n",
      "Timestep:  67\n",
      "Episode:  386\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "79/79 [==============================] - 0s 164us/step - loss: 17.8364 - accuracy: 0.0000e+00\n",
      "Timestep:  79\n",
      "Episode:  387\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.3284 - accuracy: 0.6400\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  388\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "119/119 [==============================] - 0s 109us/step - loss: 11.4914 - accuracy: 0.0000e+00\n",
      "Timestep:  119\n",
      "Episode:  389\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "214/214 [==============================] - 0s 103us/step - loss: 10.1657 - accuracy: 0.0000e+00\n",
      "Timestep:  214\n",
      "Episode:  390\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 123us/step - loss: 23.5278 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  391\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.0885 - accuracy: 0.3267\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  392\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.9368 - accuracy: 0.4567\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  393\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "92/92 [==============================] - 0s 119us/step - loss: 20.7163 - accuracy: 0.0000e+00\n",
      "Timestep:  92\n",
      "Episode:  394\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.8080 - accuracy: 0.5133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  395\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "65/65 [==============================] - 0s 154us/step - loss: 31.1453 - accuracy: 0.0000e+00\n",
      "Timestep:  65\n",
      "Episode:  396\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 2.1811 - accuracy: 0.3100\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  397\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "60/60 [==============================] - 0s 133us/step - loss: 26.8875 - accuracy: 0.0000e+00\n",
      "Timestep:  60\n",
      "Episode:  398\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.7508 - accuracy: 0.2833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  399\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 102us/step - loss: 0.9463 - accuracy: 0.2633\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  400\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 1.0631 - accuracy: 0.2867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  401\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.2684 - accuracy: 0.7033\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  402\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "160/160 [==============================] - 0s 125us/step - loss: 14.3707 - accuracy: 0.0000e+00\n",
      "Timestep:  160\n",
      "Episode:  403\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "286/286 [==============================] - 0s 99us/step - loss: 7.3304 - accuracy: 0.0000e+00\n",
      "Timestep:  286\n",
      "Episode:  404\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 1.4371 - accuracy: 0.2233\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  405\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "80/80 [==============================] - 0s 125us/step - loss: 25.7234 - accuracy: 0.0000e+00\n",
      "Timestep:  80\n",
      "Episode:  406\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "147/147 [==============================] - 0s 116us/step - loss: 18.7662 - accuracy: 0.0000e+00\n",
      "Timestep:  147\n",
      "Episode:  407\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.2738 - accuracy: 0.7200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  408\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "178/178 [==============================] - 0s 112us/step - loss: 12.2648 - accuracy: 0.0000e+00\n",
      "Timestep:  178\n",
      "Episode:  409\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "138/138 [==============================] - 0s 109us/step - loss: 14.2894 - accuracy: 0.0000e+00\n",
      "Timestep:  138\n",
      "Episode:  410\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "60/60 [==============================] - 0s 150us/step - loss: 23.8970 - accuracy: 0.0000e+00\n",
      "Timestep:  60\n",
      "Episode:  411\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 101us/step - loss: 0.5037 - accuracy: 0.6900\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  412\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.8477 - accuracy: 0.3100\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  413\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "91/91 [==============================] - 0s 121us/step - loss: 19.3783 - accuracy: 0.0000e+00\n",
      "Timestep:  91\n",
      "Episode:  414\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.4073 - accuracy: 0.3267\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  415\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "282/282 [==============================] - 0s 103us/step - loss: 8.0324 - accuracy: 0.0000e+00\n",
      "Timestep:  282\n",
      "Episode:  416\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "59/59 [==============================] - 0s 119us/step - loss: 28.2243 - accuracy: 0.0000e+00\n",
      "Timestep:  59\n",
      "Episode:  417\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.4339 - accuracy: 0.4833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  418\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 123us/step - loss: 29.0305 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  419\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 0.5657 - accuracy: 0.6300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  420\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 117us/step - loss: 20.5821 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  421\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 28.0873 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  422\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.3953 - accuracy: 0.5833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  423\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "80/80 [==============================] - 0s 112us/step - loss: 18.8490 - accuracy: 0.0000e+00\n",
      "Timestep:  80\n",
      "Episode:  424\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "97/97 [==============================] - 0s 124us/step - loss: 19.5189 - accuracy: 0.0000e+00\n",
      "Timestep:  97\n",
      "Episode:  425\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "138/138 [==============================] - 0s 109us/step - loss: 12.2274 - accuracy: 0.0000e+00\n",
      "Timestep:  138\n",
      "Episode:  426\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 143us/step - loss: 27.1112 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  427\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "76/76 [==============================] - 0s 118us/step - loss: 18.8446 - accuracy: 0.0000e+00\n",
      "Timestep:  76\n",
      "Episode:  428\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "139/139 [==============================] - 0s 108us/step - loss: 17.9075 - accuracy: 0.0000e+00\n",
      "Timestep:  139\n",
      "Episode:  429\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 101us/step - loss: 0.2985 - accuracy: 0.6433\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  430\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "81/81 [==============================] - 0s 136us/step - loss: 19.2651 - accuracy: 0.0000e+00\n",
      "Timestep:  81\n",
      "Episode:  431\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "90/90 [==============================] - 0s 122us/step - loss: 17.8220 - accuracy: 0.0000e+00\n",
      "Timestep:  90\n",
      "Episode:  432\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 23.7497 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  433\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "154/154 [==============================] - 0s 117us/step - loss: 11.6595 - accuracy: 0.0000e+00\n",
      "Timestep:  154\n",
      "Episode:  434\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "175/175 [==============================] - 0s 108us/step - loss: 9.1405 - accuracy: 0.0000e+00\n",
      "Timestep:  175\n",
      "Episode:  435\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 98us/step - loss: 0.2101 - accuracy: 0.7700\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  436\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "78/78 [==============================] - 0s 128us/step - loss: 20.5618 - accuracy: 0.0000e+00\n",
      "Timestep:  78\n",
      "Episode:  437\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 105us/step - loss: 0.5022 - accuracy: 0.6133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  438\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.4221 - accuracy: 0.5800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  439\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "101/101 [==============================] - 0s 109us/step - loss: 15.8914 - accuracy: 0.0000e+00\n",
      "Timestep:  101\n",
      "Episode:  440\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "103/103 [==============================] - 0s 116us/step - loss: 15.9607 - accuracy: 0.0000e+00\n",
      "Timestep:  103\n",
      "Episode:  441\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 2.2811 - accuracy: 0.1633\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  442\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "76/76 [==============================] - 0s 131us/step - loss: 18.0565 - accuracy: 0.0000e+00\n",
      "Timestep:  76\n",
      "Episode:  443\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "70/70 [==============================] - 0s 114us/step - loss: 22.3140 - accuracy: 0.0000e+00\n",
      "Timestep:  70\n",
      "Episode:  444\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.3822 - accuracy: 0.4667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  445\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "85/85 [==============================] - 0s 118us/step - loss: 16.8101 - accuracy: 0.0000e+00\n",
      "Timestep:  85\n",
      "Episode:  446\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 102us/step - loss: 0.9262 - accuracy: 0.5067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  447\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "69/69 [==============================] - 0s 130us/step - loss: 22.5908 - accuracy: 0.0000e+00\n",
      "Timestep:  69\n",
      "Episode:  448\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.6682 - accuracy: 0.4200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  449\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "279/279 [==============================] - 0s 100us/step - loss: 8.1873 - accuracy: 0.0000e+00\n",
      "Timestep:  279\n",
      "Episode:  450\n",
      "=====================================\n",
      "Model saved\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.5850 - accuracy: 0.5133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  451\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "83/83 [==============================] - 0s 120us/step - loss: 16.8479 - accuracy: 0.0000e+00\n",
      "Timestep:  83\n",
      "Episode:  452\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "242/242 [==============================] - 0s 107us/step - loss: 8.2343 - accuracy: 0.0000e+00\n",
      "Timestep:  242\n",
      "Episode:  453\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 1.5808 - accuracy: 0.3300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  454\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.8954 - accuracy: 0.3967\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  455\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.3552 - accuracy: 0.6800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  456\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.2264 - accuracy: 0.7467\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  457\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.6648 - accuracy: 0.4633\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  458\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "61/61 [==============================] - 0s 131us/step - loss: 23.1011 - accuracy: 0.0000e+00\n",
      "Timestep:  61\n",
      "Episode:  459\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 102us/step - loss: 2.0116 - accuracy: 0.3533\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  460\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 0.3457 - accuracy: 0.6433\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  461\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "177/177 [==============================] - 0s 103us/step - loss: 10.6741 - accuracy: 0.0000e+00\n",
      "Timestep:  177\n",
      "Episode:  462\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.4117 - accuracy: 0.4767\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  463\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 100us/step - loss: 1.1741 - accuracy: 0.3400\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  464\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 104us/step - loss: 1.5311 - accuracy: 0.2867\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  465\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "71/71 [==============================] - 0s 127us/step - loss: 20.1207 - accuracy: 0.0000e+00\n",
      "Timestep:  71\n",
      "Episode:  466\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 97us/step - loss: 1.8553 - accuracy: 0.2967\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  467\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.5614 - accuracy: 0.2700\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  468\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 125us/step - loss: 28.1432 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  469\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.7437 - accuracy: 0.3133\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  470\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 0.8224 - accuracy: 0.2800\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  471\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "242/242 [==============================] - 0s 120us/step - loss: 7.7258 - accuracy: 0.0000e+00\n",
      "Timestep:  242\n",
      "Episode:  472\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 127us/step - loss: 0.6578 - accuracy: 0.5367\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  473\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "290/290 [==============================] - 0s 100us/step - loss: 7.5485 - accuracy: 0.0000e+00\n",
      "Timestep:  290\n",
      "Episode:  474\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 143us/step - loss: 29.6251 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  475\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 0.4322 - accuracy: 0.5500\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  476\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "79/79 [==============================] - 0s 152us/step - loss: 21.2349 - accuracy: 0.0000e+00\n",
      "Timestep:  79\n",
      "Episode:  477\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "56/56 [==============================] - 0s 161us/step - loss: 26.8125 - accuracy: 0.0000e+00\n",
      "Timestep:  56\n",
      "Episode:  478\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "265/265 [==============================] - 0s 117us/step - loss: 6.6933 - accuracy: 0.0000e+00\n",
      "Timestep:  265\n",
      "Episode:  479\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "101/101 [==============================] - 0s 119us/step - loss: 22.7250 - accuracy: 0.0000e+00\n",
      "Timestep:  101\n",
      "Episode:  480\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.3693 - accuracy: 0.5300\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  481\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "79/79 [==============================] - 0s 139us/step - loss: 24.9876 - accuracy: 0.0000e+00\n",
      "Timestep:  79\n",
      "Episode:  482\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 175us/step - loss: 31.9007 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  483\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "87/87 [==============================] - 0s 138us/step - loss: 20.1941 - accuracy: 0.0000e+00\n",
      "Timestep:  87\n",
      "Episode:  484\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 175us/step - loss: 26.7481 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  485\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "110/110 [==============================] - 0s 118us/step - loss: 17.7684 - accuracy: 0.0000e+00\n",
      "Timestep:  110\n",
      "Episode:  486\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 123us/step - loss: 1.0688 - accuracy: 0.2333\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  487\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "126/126 [==============================] - 0s 103us/step - loss: 13.7969 - accuracy: 0.0000e+00\n",
      "Timestep:  126\n",
      "Episode:  488\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.6259 - accuracy: 0.5067\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  489\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "237/237 [==============================] - 0s 105us/step - loss: 6.8102 - accuracy: 0.0000e+00\n",
      "Timestep:  237\n",
      "Episode:  490\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.1239 - accuracy: 0.4667\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  491\n",
      "=====================================\n",
      "Player 2 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 142us/step - loss: 23.6769 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  492\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "57/57 [==============================] - 0s 140us/step - loss: 26.6821 - accuracy: 0.0000e+00\n",
      "Timestep:  57\n",
      "Episode:  493\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "192/192 [==============================] - 0s 109us/step - loss: 12.9839 - accuracy: 0.0000e+00\n",
      "Timestep:  192\n",
      "Episode:  494\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "77/77 [==============================] - 0s 130us/step - loss: 19.2734 - accuracy: 0.0000e+00\n",
      "Timestep:  77\n",
      "Episode:  495\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 103us/step - loss: 1.6904 - accuracy: 0.5200\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  496\n",
      "=====================================\n",
      "Player 1 scored\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "146/146 [==============================] - 0s 130us/step - loss: 15.5109 - accuracy: 0.0000e+00\n",
      "Timestep:  146\n",
      "Episode:  497\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 107us/step - loss: 1.9778 - accuracy: 0.2833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  498\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 113us/step - loss: 0.4550 - accuracy: 0.5400\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  499\n",
      "=====================================\n",
      "Epoch 1/1\n",
      "0.0001\n",
      "300/300 [==============================] - 0s 110us/step - loss: 0.3079 - accuracy: 0.6833\n",
      "No one won.\n",
      "Timestep:  300\n",
      "Episode:  500\n",
      "=====================================\n",
      "Model saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c38f6be92abc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescrete_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0maction_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescrete_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_agent_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0maction_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_to_action_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[0maction_label_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_to_action_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2ff9347c5ed2>\u001b[0m in \u001b[0;36maction_to_action_label\u001b[1;34m(action)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0ma_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactionspace\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactionspace_numbers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiscount_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_rewards, mean_steps = [], []\n",
    "player1_won, player2_won, no_one_won = 0, 0, 0\n",
    "basic = lh.BasicOpponent()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(sess.graph)\n",
    "#     tf.train.Saver().restore(sess, \"./models/model.ckpt\")\n",
    "    tf.train.Saver().save(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        episode_rewards = []\n",
    "        observations, observations_2, action_labels, action_labels_2 = [], [], [], []\n",
    "        for i in range(policy_dim):\n",
    "            action_labels.append([])\n",
    "            action_labels_2.append([])\n",
    "        \n",
    "        observation = env.reset()\n",
    "        if env.unwrapped.spec is None:\n",
    "            obs_agent_2 = env.obs_agent_two()\n",
    "\n",
    "        for step in range(1, max_steps + 1):\n",
    "            \n",
    "#             env.render()\n",
    "\n",
    "            if not IMITATION:\n",
    "                action_label = []\n",
    "                for i in range(policy_dim):\n",
    "                    action_vector = sess.run(action_distribution[i], feed_dict={input_: observation.reshape(1, -1)})[0]\n",
    "                    action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "                    # Compute action label for passing to the tf.gather_nd()\n",
    "                    action_labels[i].append([0, action_label[i]])\n",
    "\n",
    "                action = action_label_to_action(action_label)\n",
    "\n",
    "                # Same process for second player in LaserHockey\n",
    "                if env.unwrapped.spec is None:\n",
    "#                     action_label = []\n",
    "#                     for i in range(policy_dim):\n",
    "#                         action_vector = sess.run(action_distribution[i], feed_dict={input_: obs_agent_2.reshape(1, -1)})[0]\n",
    "#                         action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "#                         action_labels_2[i].append([0, action_label[i]])\n",
    "\n",
    "#                     action_2 = action_label_to_action(action_label)\n",
    "#                     observations_2.append(obs_agent_2)\n",
    "                    \n",
    "                    action_2 = basic.act(obs_agent_2)\n",
    "            \n",
    "            else:\n",
    "                observations_2.append(obs_agent_2)\n",
    "                action = descrete_action(basic.act(observation))\n",
    "                action_2 = descrete_action(basic.act(obs_agent_2))\n",
    "                action_label = action_to_action_label(action)\n",
    "                action_label_2 = action_to_action_label(action_2)\n",
    "                for i in range(policy_dim):\n",
    "                        action_labels[i].append([0, action_label[i]])\n",
    "                        action_labels_2[i].append([0, action_label_2[i]])\n",
    "                        \n",
    "            observations.append(observation)\n",
    "            \n",
    "            if env.unwrapped.spec is None:\n",
    "                observation, reward, done, info = env.step(np.hstack([action, action_2]))\n",
    "                obs_agent_2 = env.obs_agent_two()\n",
    "                \n",
    "            else: observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Optimization process\n",
    "            if step % T == 0 or done or step == max_steps:\n",
    "                T_new = step % T\n",
    "                if T_new == 0: T_new = T\n",
    "                \n",
    "                if IMITATION: winner = info.get('winner')\n",
    "                else: winner = 0\n",
    "                \n",
    "                \n",
    "                if not (IMITATION and WINNER) or winner == 1:\n",
    "                    # Calculate value and advantage functions with no lambda-return as alternative\n",
    "                    returns = discount_rewards(episode_rewards[-T_new:], gamma)\n",
    "#                     normalized_returns = returns - np.mean(returns)\n",
    "#                     normalized_returns /= np.std(normalized_returns)\n",
    "                    advantages = subtract_baseline(observations, returns)\n",
    "#                     advantages = np.ones_like(returns)\n",
    "                    clipped_advantages = clip_advantages(advantages)\n",
    "        \n",
    "#                     print(returns)\n",
    "#                     print(advantages)\n",
    "#                     print(observations)\n",
    "\n",
    "                    # Calculate td-error and generalized advantage estimation\n",
    "#                     TD_errors = td_error(observations[-step:], episode_rewards[-T_new:])\n",
    "#                     GAE = discount_rewards(TD_errors, gamma * lambda_)\n",
    "#                     prediction = sess.run(value, feed_dict={input_: observations[-1].reshape(1, -1)})\n",
    "#                     GAE = np.append(GAE, episode_rewards[-1] - prediction)\n",
    "#                     GAE = np.append(GAE, episode_rewards[-1] - critic.predict(observations[-1].reshape(1, -1))[0])\n",
    "\n",
    "                    # Optimize\n",
    "#                     actor_loss = actor.fit([observations, advantages, clipped_advantages], [action], batch_size=critic_bs)\n",
    "                    critic_loss = critic.fit(np.array(observations), [returns], batch_size=critic_bs, callbacks=callbacks_list)\n",
    "\n",
    "                    # Couldn't find a better way of implementing batch sizes in tensorflow\n",
    "                    for i in range(math.ceil(T_new/actor_bs)):\n",
    "                        temp_labels = []\n",
    "                        for j in range(policy_dim):\n",
    "                            temp_labels.append(action_labels[j][actor_bs*i:actor_bs*(i+1)])\n",
    "                        temp_action_labels = match_action_labels(temp_labels)\n",
    "\n",
    "                        sess.run(optimizer, feed_dict={input_: observations[actor_bs*i:actor_bs*(i+1)],\n",
    "                                                       action_label_: temp_action_labels,\n",
    "                                                       advantage_: advantages[actor_bs*i:actor_bs*(i+1)],\n",
    "                                                       clipped_advantage_: clipped_advantages[actor_bs*i:actor_bs*(i+1)]})\n",
    "                        \n",
    "                \n",
    "                # Same process for second player in LaserHockey\n",
    "#                 if (not (IMITATION and WINNER) or winner == -1) and env.unwrapped.spec is None:\n",
    "#                     returns = discount_rewards([-x for x in episode_rewards][-T_new:], gamma)\n",
    "#                     advantages = subtract_baseline(observations_2, returns)\n",
    "# #                     advantages = np.ones_like(returns)\n",
    "#                     clipped_advantages = clip_advantages(advantages)\n",
    "\n",
    "# #                     print(returns)\n",
    "# #                     print(advantages)\n",
    "# #                     print(len(observations_2))\n",
    "\n",
    "#                     critic_loss = critic.fit(np.array(observations_2), [returns], batch_size=critic_bs, callbacks=callbacks_list)\n",
    "\n",
    "#                     for i in range(math.ceil(T_new/actor_bs)):\n",
    "#                         temp_labels = []\n",
    "#                         for j in range(policy_dim):\n",
    "#                             temp_labels.append(action_labels_2[j][actor_bs*i:actor_bs*(i+1)])\n",
    "#                         temp_action_labels = match_action_labels(temp_labels)\n",
    "\n",
    "#                         sess.run(optimizer, feed_dict={input_: observations_2[actor_bs*i:actor_bs*(i+1)],\n",
    "#                                                        action_label_: temp_action_labels,\n",
    "#                                                        advantage_: advantages[actor_bs*i:actor_bs*(i+1)],\n",
    "#                                                        clipped_advantage_: clipped_advantages[actor_bs*i:actor_bs*(i+1)]})\n",
    "\n",
    "            \n",
    "            \n",
    "            if step % T == 0 and not (done or step == max_steps):\n",
    "                # Flush memory\n",
    "                observations, observations_2, action_labels, action_labels_2 = [], [], [], []\n",
    "                for i in range(policy_dim):\n",
    "                    action_labels.append([])\n",
    "                    action_labels_2.append([])\n",
    "                    \n",
    "            if done or step == max_steps:\n",
    "                # Print rewards for solo environment or info for Laserhockey\n",
    "                if env.unwrapped.spec is None:\n",
    "                    mean_steps.append(step)\n",
    "                    if info.get('winner') == 1: player1_won += 1\n",
    "                    elif info.get('winner') == -1: player2_won += 1\n",
    "                    else: no_one_won += 1; print('No one won.')\n",
    "                \n",
    "                all_rewards.append(np.sum(episode_rewards))    \n",
    "                print(\"Timestep: \", step)\n",
    "                print(\"Episode: \", episode)\n",
    "                \n",
    "                if env.unwrapped.spec is not None:\n",
    "                    print(\"Reward: \", all_rewards[-1])\n",
    "                    print(\"Max reward so far: \", np.max(all_rewards))\n",
    "                \n",
    "                \n",
    "                # Add reward and losses to Tensorboard\n",
    "                if not (IMITATION and WINNER) or winner != 0:\n",
    "                    new_action_labels = match_action_labels(action_labels)\n",
    "                    summary_ = sess.run(summary, feed_dict={input_: observations,\n",
    "                                                            action_label_: new_action_labels,\n",
    "                                                            advantage_: advantages,\n",
    "                                                            clipped_advantage_: clipped_advantages,\n",
    "                                                            reward_: all_rewards[-1],\n",
    "#                                                             actor_loss_: actor_loss.history['loss'][-1],\n",
    "                                                            critic_loss_: critic_loss.history['loss'][-1]})\n",
    "                    writer.add_summary(summary_, episode)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                print(\"=====================================\")\n",
    "                \n",
    "                break\n",
    "        \n",
    "        \n",
    "        # Save Model every 50 episodes\n",
    "        if episode % 50 == 0:\n",
    "            tf.train.Saver().save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "    tf.train.Saver().save(sess, \"./models/model.ckpt\")\n",
    "    env.close()\n",
    "    \n",
    "    if env.unwrapped.spec is None:\n",
    "        print('Player 1 won: ' + str(player1_won) + '/' + str(max_episodes))\n",
    "        print('Player 2 won: ' + str(player2_won) + '/' + str(max_episodes))\n",
    "        print('No one won: ' + str(no_one_won) + '/' + str(max_episodes))\n",
    "        print('Mean steps: ' + str(mean_list(mean_steps)))\n",
    "        print(mean_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function plot at 0x00000168B15C4438>\n",
      "<function plot at 0x00000168B15C4438>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, = plt.plot(np.arange(len(all_rewards)), all_rewards)\n",
    "print(plt.plot)\n",
    "print(plt.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Controls:\n",
      " left:\t\t\tleft arrow key left\n",
      " right:\t\t\tarrow key right\n",
      " up:\t\t\tarrow key up\n",
      " down:\t\t\tarrow key down\n",
      " tilt clockwise:\td\n",
      " tilt anti-clockwise:\ta\n"
     ]
    }
   ],
   "source": [
    "human = lh.HumanOpponent(env=env, player=1)\n",
    "basic = lh.BasicOpponent()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Show agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1116..1400 -> 284-tiles track\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\runlist.py\u001b[0m in \u001b[0;36mranges\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmin_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                     \u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-abcfb601a9d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtotal_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobs_agent_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs_agent_two\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    348\u001b[0m             self.score_label = pyglet.text.Label('0000', font_size=36,\n\u001b[0;32m    349\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWINDOW_H\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m40.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 color=(255,255,255,255))\n\u001b[0m\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, font_name, font_size, bold, italic, color, x, y, width, height, anchor_x, anchor_y, align, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    431\u001b[0m         super(Label, self).__init__(document, x, y, width, height, \n\u001b[0;32m    432\u001b[0m                                     \u001b[0manchor_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                                     multiline, dpi, batch, group)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         self.document.set_style(0, len(self.document.text), {\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, x, y, width, height, anchor_x, anchor_y, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    255\u001b[0m                                             \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                                             \u001b[0mmultiline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                             dpi=dpi, batch=batch, group=group)\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, width, height, multiline, dpi, batch, group, wrap_lines)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wrap_lines_invariant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_set_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_handlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     document = property(_get_document, _set_document,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_init_document\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_uninit_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mcolors_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_style_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0mlen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         \u001b[0mglyphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_glyphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m         \u001b[0mowner_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_owner_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner_runs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_glyphs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m             self._document.get_element_runs()))\n\u001b[0;32m   1011\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1012\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m                 \u001b[0mglyphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_InlineElementBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "if env.unwrapped.spec is None:\n",
    "    human = lh.HumanOpponent(env=env, player=1)\n",
    "    basic = lh.BasicOpponent()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    rewards, mean_steps = [], []\n",
    "    player1_won, player2_won, no_one_won = 0, 0, 0\n",
    "\n",
    "    # Load the model\n",
    "#     tf.train.Saver().restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    max_test_episodes = 1\n",
    "    max_test_steps = 400\n",
    "    eps = 0\n",
    "    \n",
    "    for episode in range(1, max_test_episodes + 1):\n",
    "        total_rewards = 0\n",
    "\n",
    "        observation = env.reset()\n",
    "        if env.unwrapped.spec is None: obs_agent_2 = env.obs_agent_two()\n",
    "            \n",
    "        for step in range(1, max_test_steps + 1):\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            if random.random() < eps:\n",
    "                action = random.choice(actionspace)\n",
    "                action_2 = random.choice(actionspace)\n",
    "            else:\n",
    "#                 action_label = []\n",
    "#                 for i in range(policy_dim):\n",
    "#                     action_vector = sess.run(action_distribution[i], feed_dict={input_: observation.reshape(1, -1)})[0]\n",
    "#                     action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "#                 action = action_label_to_action(action_label)\n",
    "                action = human.act(observation)\n",
    "\n",
    "                # Same process for second player in LaserHockey\n",
    "                if env.unwrapped.spec is None:\n",
    "#                     action_label = []\n",
    "#                     for i in range(policy_dim):\n",
    "#                         action_vector = sess.run(action_distribution[i], feed_dict={input_: obs_agent_2.reshape(1, -1)})[0]\n",
    "#                         action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "#                     action_2 = action_label_to_action(action_label)\n",
    "            \n",
    "#                     action = basic.act(observation)\n",
    "                    action_2 = basic.act(obs_agent_2)\n",
    "#                     action = descrete_action(basic.act(observation))\n",
    "#                     action_2 = descrete_action(basic.act(obs_agent_2))\n",
    "            \n",
    "            # np.hstack for LaserHockey environment\n",
    "            if env.unwrapped.spec == None:\n",
    "                \n",
    "                observation, reward, done, info = env.step(np.hstack([action, action_2]))\n",
    "                obs_agent_2 = env.obs_agent_two()\n",
    "                \n",
    "            else: observation, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done or step == max_test_steps:\n",
    "                # Print reward for solo environment or info for Laserhockey\n",
    "                if env.unwrapped.spec is None:\n",
    "                    mean_steps.append(step)\n",
    "                    if info.get('winner') == 1: player1_won += 1\n",
    "                    elif info.get('winner') == -1: player2_won += 1\n",
    "                    else: no_one_won += 1; print('No one won.')\n",
    "                \n",
    "                print(\"Timestep: \", step)\n",
    "                print(\"Episode: \", episode)\n",
    "                \n",
    "                if env.unwrapped.spec is not None:\n",
    "                    rewards.append(total_rewards)\n",
    "                    print(\"Reward: \", total_rewards)\n",
    "                    \n",
    "                print(\"========================================\")\n",
    "                    \n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "    if env.unwrapped.spec is None:\n",
    "        print('Player 1 won: ' + str(player1_won) + '/' + str(max_test_episodes))\n",
    "        print('Player 2 won: ' + str(player2_won) + '/' + str(max_test_episodes))\n",
    "        print('No one won: ' + str(no_one_won) + '/' + str(max_test_episodes))\n",
    "        print('Mean steps: ' + str(mean_list(mean_steps)))\n",
    "        print(mean_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1203..1508 -> 305-tiles track\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\runlist.py\u001b[0m in \u001b[0;36mranges\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmin_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                     \u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-89910364cb49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CarRacing-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys_to_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\utils\\play.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(env, transpose, fps, zoom, callback, keys_to_action)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mIf\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mkey_to_action\u001b[0m \u001b[0mmapping\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0menv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprovided\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \"\"\"\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mrendered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    348\u001b[0m             self.score_label = pyglet.text.Label('0000', font_size=36,\n\u001b[0;32m    349\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWINDOW_H\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m40.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 color=(255,255,255,255))\n\u001b[0m\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, font_name, font_size, bold, italic, color, x, y, width, height, anchor_x, anchor_y, align, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    431\u001b[0m         super(Label, self).__init__(document, x, y, width, height, \n\u001b[0;32m    432\u001b[0m                                     \u001b[0manchor_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                                     multiline, dpi, batch, group)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         self.document.set_style(0, len(self.document.text), {\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, x, y, width, height, anchor_x, anchor_y, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    255\u001b[0m                                             \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                                             \u001b[0mmultiline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                             dpi=dpi, batch=batch, group=group)\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, width, height, multiline, dpi, batch, group, wrap_lines)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wrap_lines_invariant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_set_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_handlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     document = property(_get_document, _set_document,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_init_document\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_uninit_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mcolors_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_style_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0mlen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         \u001b[0mglyphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_glyphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m         \u001b[0mowner_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_owner_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner_runs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_glyphs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m             self._document.get_element_runs()))\n\u001b[0;32m   1011\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1012\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m                 \u001b[0mglyphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_InlineElementBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "from gym.utils import play\n",
    "\n",
    "\n",
    "key_mapping = {(97,108): 3, (100,108): 1, (108,119): 0, (108,115): 2}\n",
    "\n",
    "\n",
    "def callback(obs_t, obs_tp1, action, rew, done, info):\n",
    "    return [rew,]\n",
    "plotter = play.PlayPlot(callback, 30 * 1, [\"reward\"])\n",
    "\n",
    "\n",
    "env = gym.make('CarRacing-v0')\n",
    "play.play(env, callback=None, keys_to_action=key_mapping)\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1040..1312 -> 272-tiles track\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\runlist.py\u001b[0m in \u001b[0;36mranges\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmin_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                     \u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-03ffa551ce5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CarRacing-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    348\u001b[0m             self.score_label = pyglet.text.Label('0000', font_size=36,\n\u001b[0;32m    349\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWINDOW_H\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m40.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 color=(255,255,255,255))\n\u001b[0m\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, font_name, font_size, bold, italic, color, x, y, width, height, anchor_x, anchor_y, align, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    431\u001b[0m         super(Label, self).__init__(document, x, y, width, height, \n\u001b[0;32m    432\u001b[0m                                     \u001b[0manchor_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                                     multiline, dpi, batch, group)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         self.document.set_style(0, len(self.document.text), {\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, x, y, width, height, anchor_x, anchor_y, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    255\u001b[0m                                             \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                                             \u001b[0mmultiline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                             dpi=dpi, batch=batch, group=group)\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, width, height, multiline, dpi, batch, group, wrap_lines)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wrap_lines_invariant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_set_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_handlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     document = property(_get_document, _set_document,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_init_document\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_uninit_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mcolors_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_style_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0mlen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         \u001b[0mglyphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_glyphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m         \u001b[0mowner_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_owner_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner_runs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_glyphs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m             self._document.get_element_runs()))\n\u001b[0;32m   1011\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1012\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m                 \u001b[0mglyphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_InlineElementBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CarRacing-v0')\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = {[1,2]:0}\n",
    "ord('w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

optimizations
    add transformer
        can I predict for multiple obs in one pass?
            actor: no
            learner from memory: yes!
                crypto: add profit state to each kline
        set min_sequence_length
        environment?
            use architecture parameter
            pass obs as sequence with past of maximally min_sequence_length, not seperately
        data collector
            also pass obs as sequences
        actor
            repeat first obs to fill min_sequence_length
        learner and/or data collector
            add min_sequence_length - 1 times the first obs at the beginning of the sequence for gym
            forward pass and train on whole sequence
    optimize whole code repository
        how come some gpu memory is given back after execution, and some not?
        bandits
            predefine numpy arrays, calculate vectorized
        data collector
            save state for continued training
            when loading, collecting samples is no longer needed
        learner
    deepmind papers
    saved workona papers
    batch size finder

runs
    laserhockey
        max_steps = 1000, reward = 50
        increase actors
        maybe increase batch size & lr and decrease d_target
training plots:
combined loss (1,2)
V-loss (1,2)
Q-loss (1,2)
Ï€-loss (1,2)
average performance (real or not?)

validation plots:
index evaluation average 
bandits average evaluation performance
[argmax performance (1,2,1+2)]

use old code to generate newer one
use old versions to merge with specified functionalities

src structure:
config file
[parameter server] (is just saved weights on ssd)
data collector
    [stepwise data]
        [add sequence nicely]
    [sequential data]
        [reorganize dict list]
    [data types]
        [observations]
        [value, advantage (1 & 2)]
        [index, policy]
        [chosen action and probability]
        [reward, return]
        [terminated, truncated]
architectures
    [dense]
    transformer
[actor class]
learner class
    [where to save and how, what name]
    get data from data collector
    estimate all off-policy values
    update weights
    [push weights if needed]
    return losses for tracking
    learning rate finder
    continue training where left of
[bandit class]
training class
    run learner code
    check actor pull function
    write metrics
    make it available to load checkpoints and continue training where left off
metric class
    get from data collector
    get from learner
    get from environment?
    get from actor?
    get from bandit?
environment class?
    load correct env
    handle whatever env specific needs to be done
    discretisize continuous envs in gym, when passing actions
    laserhockey
        add basic agent into env
        preprocess observations
        discretisize actions and handle and convert them
        get dense rewards from info
    crypto
        check if multiple heads are possible in gym
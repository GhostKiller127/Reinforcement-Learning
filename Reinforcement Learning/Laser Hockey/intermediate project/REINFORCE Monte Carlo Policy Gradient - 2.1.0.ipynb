{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.066765Z",
     "start_time": "2020-03-29T23:59:15.847033Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ghost\\.conda\\envs\\openai_gym\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# from tensorflow.contrib.layers import fully_connected as dense\n",
    "from tensorflow.compat.v1.layers import dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import laser_hockey_env as lh\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:47:28.861725Z",
     "start_time": "2020-03-31T17:47:28.856730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "# env = gym.make('CarRacing-v0')\n",
    "# env = lh.LaserHockeyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.108735Z",
     "start_time": "2020-03-29T23:59:19.097742Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = {[1,2]:0}\n",
    "ord('w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.113732Z",
     "start_time": "2020-03-29T23:59:19.110733Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from gym.utils import play\n",
    "\n",
    "\n",
    "# key_mapping = {(97,108): 3, (100,108): 1, (108,119): 0, (108,115): 2}\n",
    "\n",
    "# # Press L and W,A,S,D for movement\n",
    "\n",
    "# def callback(obs_t, obs_tp1, action, rew, done, info):\n",
    "#     return [rew,]\n",
    "# plotter = play.PlayPlot(callback, 30 * 1, [\"reward\"])\n",
    "\n",
    "\n",
    "# play.play(env, fps=30, callback=plotter.callback, keys_to_action=key_mapping)\n",
    "# env.reset()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.119727Z",
     "start_time": "2020-03-29T23:59:19.115730Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "# for i_episode in range(1000):\n",
    "#     observation = env.reset()\n",
    "#     for t in range(500):\n",
    "# #         env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Set up hyperparameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.129721Z",
     "start_time": "2020-03-29T23:59:19.121725Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_episodes = 2000\n",
    "max_steps = 200\n",
    "\n",
    "# Hyperparameters\n",
    "actor_lr = 0.0003\n",
    "critic_lr = 0.0003\n",
    "actor_bs = 32\n",
    "critic_bs = 32\n",
    "gamma = 0.98\n",
    "hidden_size = 100\n",
    "epsilon = 0.2\n",
    "\n",
    "# standardized or normalized returns for CartPole-v0\n",
    "returns_mean = 37.965\n",
    "returns_std = 12.579\n",
    "returns_norm = 50\n",
    "\n",
    "# Amount of discrete actions per dimension for Box space\n",
    "k = 3\n",
    "\n",
    "# Not used hyperparameters\n",
    "lambda_ = 0.95\n",
    "\n",
    "# Choose REINFORCE or PPO loss function, PPO uses epsilon\n",
    "REINFORCE = True\n",
    "\n",
    "# Choose to linearly anneal the learning rate from 1 to 0 over the course of learning\n",
    "LINEAR = False\n",
    "\n",
    "# Optimize every episode or every T timesteps, depending on environment\n",
    "EPISODIC = True\n",
    "if EPISODIC: T = max_steps\n",
    "else: T = 128\n",
    "\n",
    "# Represent the discrete distribution as factorized across dimensions\n",
    "FACTORIZED = True\n",
    "\n",
    "# Choose to imitate basic agent in LaserHockey and if only for the winning cases\n",
    "IMITATION = True\n",
    "WINNER = True\n",
    "if env.unwrapped.spec is not None:\n",
    "    IMITATION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Set up Tensorboard\n",
    "To launch tensorboard : `tensorboard --logdir \"Google Drive/Reinforcement Learning/tensorboard/REINFORCE\"`\n",
    "\n",
    "`tensorboard --logdir \"Google Drive/Reinforcement Learning/tensorboard/REINFORCE/LunarLander-v2\"`\n",
    "\n",
    "`tensorboard --logdir \"tensorboard/REINFORCE\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.138713Z",
     "start_time": "2020-03-29T23:59:19.131719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Change parameters for different runs to have them seperate in Tensorboard\n",
    "# writer = tf.summary.create_file_writer('/tensorboard/REINFORCE/CartPole-v0/'\n",
    "#                                'E=' + str(max_steps) + ', T=' + str(T) +\n",
    "#                                ', lr_a=' + str(actor_lr) + ', lr_c=' + str(critic_lr) +\n",
    "#                                ', bs_a=' + str(actor_bs) + ', bs_c=' + str(critic_bs) +\n",
    "#                                ', g=' + str(gamma) + ', hs=2x' + str(hidden_size) +\n",
    "#                                ', e=0.2, vs basic 3x3')\n",
    "\n",
    "writer = tf.summary.FileWriter('/Users/ghost/Google Drive/Reinforcement Learning/tensorboard/REINFORCE/CartPole-v0/'\n",
    "                               'E=' + str(max_steps) + ', T=' + str(T) +\n",
    "                               ', lr_a=' + str(actor_lr) + ', lr_c=' + str(critic_lr) +\n",
    "                               ', bs_a=' + str(actor_bs) + ', bs_c=' + str(critic_bs) +\n",
    "                               ', g=' + str(gamma) + ', hs=2x' + str(hidden_size) +\n",
    "                               ', returns_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Show and compute specifications of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T17:47:32.109778Z",
     "start_time": "2020-03-31T17:47:32.099784Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actionspace\n",
      "=========================\n",
      "Box(1,)\n",
      "1\n",
      "[1.]\n",
      "[-1.]\n",
      "=========================\n",
      "Observationspace\n",
      "=========================\n",
      "Box(2,)\n",
      "2\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print('Actionspace')\n",
    "print('=========================')\n",
    "print(env.action_space)\n",
    "if env.action_space.shape == ():\n",
    "    print(env.action_space.n)\n",
    "else:\n",
    "    print(env.action_space.shape[0])\n",
    "    print(env.action_space.high)\n",
    "    print(env.action_space.low)\n",
    "print('=========================')\n",
    "print('Observationspace')\n",
    "print('=========================')\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.shape[0])\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.162697Z",
     "start_time": "2020-03-29T23:59:19.154702Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute dimensions for model's input/output sizes.\n",
    "observation_dim = env.observation_space.shape[0]\n",
    "    # Checking if Descrete or Box action space\n",
    "if env.action_space.shape == ():\n",
    "    a_dim, action_dim = 1, env.action_space.n\n",
    "    policy_dim = a_dim\n",
    "else:\n",
    "    # If environment is LaserHockey take only the first 3 action dimensions.\n",
    "    if env.unwrapped.spec is None: a_dim = 3\n",
    "    else: a_dim = env.action_space.shape[0]\n",
    "    if FACTORIZED: action_dim, policy_dim = k, a_dim\n",
    "    else: action_dim, policy_dim = k**a_dim, 1\n",
    "    \n",
    "    # Also compute descrete actionspace for the Box space, because the Descrete space doesn't need one.\n",
    "    actionspace = []\n",
    "    for d in range(a_dim):\n",
    "        actionspace.append(np.linspace(env.action_space.high[d], env.action_space.low[d], k))\n",
    "    actionspace = np.array(np.meshgrid(*actionspace)).reshape(a_dim, -1).T\n",
    "    actionspace_numbers = np.arange(k**a_dim)\n",
    "    actionspace_numbers = actionspace_numbers.reshape(np.full(policy_dim, action_dim))\n",
    "    \n",
    "    print('Descretized actionspace and corresponding numbers')\n",
    "    print('=================================================')\n",
    "    print(actionspace)\n",
    "    print(actionspace_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.184681Z",
     "start_time": "2020-03-29T23:59:19.164695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def descrete_action(action):\n",
    "    for i in range(3):\n",
    "        if action[i] > 0.5:\n",
    "            action[i] = 1\n",
    "        elif action[i] < -0.5:\n",
    "            action[i] = -1\n",
    "        else: action[i] = 0\n",
    "    return action\n",
    "\n",
    "def mean_list(lst):\n",
    "    if len(lst) != 0:\n",
    "        return sum(lst)/len(lst)\n",
    "    return 0\n",
    "\n",
    "def match_action_labels(action_labels):\n",
    "    for i in range(len(action_labels[0])):\n",
    "        for j in range(policy_dim):\n",
    "            action_labels[j][i][0] = i\n",
    "    return action_labels\n",
    "\n",
    "def action_label_to_action(action_label):\n",
    "    if env.action_space.shape == ():\n",
    "        return action_label[0]\n",
    "    return actionspace[actionspace_numbers.item(*action_label)]\n",
    "\n",
    "def action_to_action_label(action):\n",
    "    for i in range(k**a_dim):\n",
    "        if np.array_equal(actionspace[i], action):\n",
    "            return np.argwhere(actionspace_numbers == i).flatten()\n",
    "\n",
    "def discount_rewards(episode_r, discount):\n",
    "    value_f = 0\n",
    "    discounted_rewards = np.zeros_like(episode_r, dtype='float64')\n",
    "    for t in reversed(range(len(discounted_rewards))):\n",
    "        value_f = value_f * discount + episode_r[t]\n",
    "        discounted_rewards[t] = value_f\n",
    "    return discounted_rewards\n",
    "\n",
    "def td_error(episode_obs, episode_r):\n",
    "    td_errors = np.zeros(len(episode_r) - 1)\n",
    "    for t in range(len(td_errors)):\n",
    "        if t == (len(td_errors) - 1):\n",
    "            td_errors[t] = episode_r[t] + gamma * episode_r[-1] - critic.predict(episode_obs[t].reshape(1, -1))[0]\n",
    "        else:\n",
    "            td_errors[t] = episode_r[t] + gamma * critic.predict(episode_obs[t+1].reshape(1, -1))[0] - critic.predict(episode_obs[t].reshape(1, -1))[0]\n",
    "    return td_errors\n",
    "\n",
    "def subtract_baseline(episode_obs, value_f):\n",
    "    advantages = np.zeros_like(value_f)\n",
    "    for t in range(len(advantages)):\n",
    "#         advantages[t] = 1.0\n",
    "        advantages[t] = value_f[t] - critic.predict(episode_obs[t].reshape(1, -1))[0]\n",
    "    return advantages\n",
    "\n",
    "def clip_advantages(advantages):\n",
    "    clipped_advantages = np.zeros_like(advantages)\n",
    "    for i in range(len(clipped_advantages)):\n",
    "        if advantages[i] >= 0:\n",
    "            clipped_advantages[i] = (1 + epsilon) * advantages[i]\n",
    "        else: clipped_advantages[i] = (1 - epsilon) * advantages[i]\n",
    "    return clipped_advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.188678Z",
     "start_time": "2020-03-29T23:59:19.186679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Create Policy Gradient or PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.424512Z",
     "start_time": "2020-03-29T23:59:19.191679Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-d30afa9dc1b7>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ghost\\.conda\\envs\\openai_gym\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"inputs\"):    \n",
    "    # Placeholders for Tensorboard\n",
    "    reward_ = tf.placeholder(tf.float32, name='reward')\n",
    "#     actor_loss_ = tf.placeholder(tf.float32, name='actor_loss')\n",
    "    critic_loss_ = tf.placeholder(tf.float32, name='critic_loss')\n",
    "    \n",
    "    # Placeholders for training\n",
    "    input_ = tf.placeholder(tf.float32, [None, observation_dim], name='input')\n",
    "    action_label_ = tf.placeholder(tf.int32, [None, None, 2], name='action_label')\n",
    "    advantage_ = tf.placeholder(tf.float32, [None,], name='advantage')\n",
    "    clipped_advantage_ = tf.placeholder(tf.float32, [None,], name='clipped_advantage')\n",
    "    \n",
    "    with tf.name_scope('policy'):\n",
    "        relu_1, relu_2, action_distribution = {}, {}, {}\n",
    "        for i in range(policy_dim):\n",
    "            relu_1[i] = dense(inputs = input_, units = hidden_size, activation = tf.nn.relu,\n",
    "                              kernel_initializer = tf.compat.v1.keras.initializers.glorot_normal())\n",
    "            relu_2[i] = dense(inputs = relu_1[i], units = hidden_size, activation = tf.nn.relu,\n",
    "                              kernel_initializer = tf.compat.v1.keras.initializers.glorot_normal())\n",
    "            action_distribution[i] = dense(inputs = relu_2[i], units = action_dim, activation = tf.nn.softmax,\n",
    "                                           kernel_initializer = tf.compat.v1.keras.initializers.glorot_normal())\n",
    "            \n",
    "#             relu_1[i] = dense(inputs = input_, num_outputs = hidden_size, activation_fn = tf.nn.relu,\n",
    "#                               weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "#             relu_2[i] = dense(inputs = relu_1[i], num_outputs = hidden_size, activation_fn = tf.nn.relu,\n",
    "#                               weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "#             action_distribution[i] = dense(inputs = relu_2[i], num_outputs = action_dim, activation_fn = tf.nn.softmax,\n",
    "#                                            weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "                \n",
    "    with tf.name_scope('policy_loss'):\n",
    "        joint_policy = tf.Variable(1.0)\n",
    "        for i in range(policy_dim):\n",
    "            joint_policy = joint_policy * tf.gather_nd(action_distribution[i], action_label_[i])\n",
    "            \n",
    "        if REINFORCE:\n",
    "            # Policy gradient objective function\n",
    "            policy_loss = tf.reduce_mean(tf.log(joint_policy + 1e-10) * advantage_)\n",
    "        else:\n",
    "            # PPO Clipped Surrogate Objective\n",
    "            old_policy = tf.stop_gradient(tf.identity(joint_policy))\n",
    "            policy_loss = tf.reduce_mean(tf.minimum(joint_policy / (old_policy + 1e-10) * advantage_, clipped_advantage_))\n",
    "    \n",
    "    with tf.name_scope('optimizers'):\n",
    "        optimizer = tf.train.AdamOptimizer(actor_lr).minimize(-policy_loss)\n",
    "        \n",
    "    \n",
    "tf.summary.scalar('Reward', reward_)\n",
    "# tf.summary.scalar('Policy_Loss', actor_loss_)\n",
    "tf.summary.scalar('Policy_Loss', policy_loss)\n",
    "tf.summary.scalar('Critic_Loss', critic_loss_)\n",
    "\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Model help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.438502Z",
     "start_time": "2020-03-29T23:59:19.426510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def actor_loss(old_prediction, advantage, clipped_advantage):\n",
    "    def loss(y_true, y_pred):\n",
    "#         new_prediction = tf.gather_nd(y_pred, tf.cast(y_true, tf.int64))\n",
    "        new_prediction = y_true * y_pred\n",
    "        if REINFORCE:\n",
    "            return -K.mean(K.log(new_prediction + 1e-10) * advantage)\n",
    "        r = new_prediction / (old_prediction + 1e-10)\n",
    "        return -K.mean(K.minimum(r * advantage, clipped_advantage))\n",
    "    return loss\n",
    "\n",
    "class decay_history(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.lr.append(linear_decay(episode))\n",
    "        print(' lr:', self.lr[-1])\n",
    "\n",
    "def linear_decay(episode):\n",
    "    decay = -(1 / max_episodes) * episode + 1\n",
    "    new_lr = critic_lr * decay\n",
    "    if LINEAR:\n",
    "        print(new_lr)\n",
    "        return new_lr\n",
    "    print(critic_lr)\n",
    "    return critic_lr\n",
    "\n",
    "decay_history = decay_history()\n",
    "new_lr = tf.keras.callbacks.LearningRateScheduler(linear_decay)\n",
    "# callbacks_list = [decay_history, new_lr]\n",
    "callbacks_list = [new_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.476476Z",
     "start_time": "2020-03-29T23:59:19.442499Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ghost\\.conda\\envs\\openai_gym\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "critic = Sequential([\n",
    "#     BatchNormalization(),\n",
    "    Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "    Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "    Dense(1, kernel_initializer='glorot_normal')])\n",
    "\n",
    "critic.compile(optimizer=Adam(lr=0.0), loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T23:59:19.482470Z",
     "start_time": "2020-03-29T23:59:19.478475Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Didn't worked yet\n",
    "\n",
    "# def build_actor():\n",
    "#     state_input = Input(shape=(observation_dim,))\n",
    "#     old_prediction = Input(shape=(1,))\n",
    "#     advantage = Input(shape=(1,))\n",
    "#     clipped_advantage = Input(shape=(1,))\n",
    "\n",
    "#     x = Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu')(state_input)\n",
    "#     x = Dense(hidden_size, kernel_initializer='glorot_normal', activation='relu')(x)\n",
    "#     action_distribution = Dense(action_dim, kernel_initializer='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input, old_prediction, advantage, clipped_advantage], outputs=[action_distribution])\n",
    "#     model.summary()\n",
    "#     model.compile(optimizer=Adam(lr=actor_lr), loss=[actor_loss(old_prediction, advantage, clipped_advantage)])\n",
    "\n",
    "#     return model\n",
    "\n",
    "# actor = build_actor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T00:10:19.285455Z",
     "start_time": "2020-03-29T23:59:19.484469Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rewards, mean_steps = [], []\n",
    "player1_won, player2_won, no_one_won = 0, 0, 0\n",
    "basic = lh.BasicOpponent()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(sess.graph)\n",
    "#     tf.train.Saver().restore(sess, \"./models/model.ckpt\")\n",
    "    tf.train.Saver().save(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        episode_rewards = []\n",
    "        observations, observations_2, action_labels, action_labels_2 = [], [], [], []\n",
    "        for i in range(policy_dim):\n",
    "            action_labels.append([])\n",
    "            action_labels_2.append([])\n",
    "        \n",
    "        observation = env.reset()\n",
    "        if env.unwrapped.spec is None:\n",
    "            obs_agent_2 = env.obs_agent_two()\n",
    "\n",
    "        for step in range(1, max_steps + 1):\n",
    "            \n",
    "#             env.render()\n",
    "\n",
    "            if not IMITATION:\n",
    "                action_label = []\n",
    "                for i in range(policy_dim):\n",
    "                    action_vector = sess.run(action_distribution[i], feed_dict={input_: observation.reshape(1, -1)})[0]\n",
    "                    action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "                    # Compute action label for passing to the tf.gather_nd()\n",
    "                    action_labels[i].append([0, action_label[i]])\n",
    "\n",
    "                action = action_label_to_action(action_label)\n",
    "\n",
    "                # Same process for second player in LaserHockey\n",
    "                if env.unwrapped.spec is None:\n",
    "#                     action_label = []\n",
    "#                     for i in range(policy_dim):\n",
    "#                         action_vector = sess.run(action_distribution[i], feed_dict={input_: obs_agent_2.reshape(1, -1)})[0]\n",
    "#                         action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "#                         action_labels_2[i].append([0, action_label[i]])\n",
    "\n",
    "#                     action_2 = action_label_to_action(action_label)\n",
    "#                     observations_2.append(obs_agent_2)\n",
    "                    \n",
    "                    action_2 = basic.act(obs_agent_2)\n",
    "            \n",
    "            else:\n",
    "                observations_2.append(obs_agent_2)\n",
    "                action = descrete_action(basic.act(observation))\n",
    "                action_2 = descrete_action(basic.act(obs_agent_2))\n",
    "                action_label = action_to_action_label(action)\n",
    "                action_label_2 = action_to_action_label(action_2)\n",
    "                for i in range(policy_dim):\n",
    "                        action_labels[i].append([0, action_label[i]])\n",
    "                        action_labels_2[i].append([0, action_label_2[i]])\n",
    "                        \n",
    "            observations.append(observation)\n",
    "            \n",
    "            if env.unwrapped.spec is None:\n",
    "                observation, reward, done, info = env.step(np.hstack([action, action_2]))\n",
    "                obs_agent_2 = env.obs_agent_two()\n",
    "                \n",
    "            else: observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Optimization process\n",
    "            if step % T == 0 or done or step == max_steps:\n",
    "                T_new = step % T\n",
    "                if T_new == 0: T_new = T\n",
    "                \n",
    "                if IMITATION: winner = info.get('winner')\n",
    "                else: winner = 0\n",
    "                \n",
    "                \n",
    "                if not (IMITATION and WINNER) or winner == 1:\n",
    "                    # Calculate value and advantage functions with no lambda-return as alternative\n",
    "                    returns = discount_rewards(episode_rewards[-T_new:], gamma)\n",
    "#                     returns_std_each = (returns-np.mean(returns))/np.std(returns)\n",
    "#                     returns_standardized = (returns-returns_mean)/returns_std\n",
    "                    returns_normalized = returns/returns_norm\n",
    "                    \n",
    "#                     print(np.mean(returns))\n",
    "#                     print(np.std(returns))\n",
    "                    print(returns)\n",
    "#                     print(returns_standardized)\n",
    "                    print(returns_normalized)\n",
    "                    \n",
    "                    returns = returns_normalized\n",
    "                    advantages = subtract_baseline(observations, returns)\n",
    "                    clipped_advantages = clip_advantages(advantages)\n",
    "    \n",
    "                    print(advantages)\n",
    "#                     print(observations)\n",
    "\n",
    "\n",
    "                    # Calculate td-error and generalized advantage estimation\n",
    "#                     TD_errors = td_error(observations[-step:], episode_rewards[-T_new:])\n",
    "#                     GAE = discount_rewards(TD_errors, gamma * lambda_)\n",
    "#                     prediction = sess.run(value, feed_dict={input_: observations[-1].reshape(1, -1)})\n",
    "#                     GAE = np.append(GAE, episode_rewards[-1] - prediction)\n",
    "#                     GAE = np.append(GAE, episode_rewards[-1] - critic.predict(observations[-1].reshape(1, -1))[0])\n",
    "\n",
    "                    # Optimize\n",
    "#                     actor_loss = actor.fit([observations, advantages, clipped_advantages], [action], batch_size=critic_bs)\n",
    "                    critic_loss = critic.fit(np.array(observations), [returns], batch_size=critic_bs, callbacks=callbacks_list)\n",
    "\n",
    "                    # Couldn't find a better way of implementing batch sizes in tensorflow\n",
    "                    for i in range(math.ceil(T_new/actor_bs)):\n",
    "                        temp_labels = []\n",
    "                        for j in range(policy_dim):\n",
    "                            temp_labels.append(action_labels[j][actor_bs*i:actor_bs*(i+1)])\n",
    "                        temp_action_labels = match_action_labels(temp_labels)\n",
    "\n",
    "                        sess.run(optimizer, feed_dict={input_: observations[actor_bs*i:actor_bs*(i+1)],\n",
    "                                                       action_label_: temp_action_labels,\n",
    "                                                       advantage_: advantages[actor_bs*i:actor_bs*(i+1)],\n",
    "                                                       clipped_advantage_: clipped_advantages[actor_bs*i:actor_bs*(i+1)]})\n",
    "                        \n",
    "                \n",
    "                # Same process for second player in LaserHockey\n",
    "#                 if (not (IMITATION and WINNER) or winner == -1) and env.unwrapped.spec is None:\n",
    "#                     returns = discount_rewards([-x for x in episode_rewards][-T_new:], gamma)\n",
    "#                     advantages = subtract_baseline(observations_2, returns)\n",
    "# #                     advantages = np.ones_like(returns)\n",
    "#                     clipped_advantages = clip_advantages(advantages)\n",
    "\n",
    "# #                     print(returns)\n",
    "# #                     print(advantages)\n",
    "# #                     print(len(observations_2))\n",
    "\n",
    "#                     critic_loss = critic.fit(np.array(observations_2), [returns], batch_size=critic_bs, callbacks=callbacks_list)\n",
    "\n",
    "#                     for i in range(math.ceil(T_new/actor_bs)):\n",
    "#                         temp_labels = []\n",
    "#                         for j in range(policy_dim):\n",
    "#                             temp_labels.append(action_labels_2[j][actor_bs*i:actor_bs*(i+1)])\n",
    "#                         temp_action_labels = match_action_labels(temp_labels)\n",
    "\n",
    "#                         sess.run(optimizer, feed_dict={input_: observations_2[actor_bs*i:actor_bs*(i+1)],\n",
    "#                                                        action_label_: temp_action_labels,\n",
    "#                                                        advantage_: advantages[actor_bs*i:actor_bs*(i+1)],\n",
    "#                                                        clipped_advantage_: clipped_advantages[actor_bs*i:actor_bs*(i+1)]})\n",
    "\n",
    "            \n",
    "            \n",
    "            if step % T == 0 and not (done or step == max_steps):\n",
    "                # Flush memory\n",
    "                observations, observations_2, action_labels, action_labels_2 = [], [], [], []\n",
    "                for i in range(policy_dim):\n",
    "                    action_labels.append([])\n",
    "                    action_labels_2.append([])\n",
    "                    \n",
    "            if done or step == max_steps:\n",
    "                # Print rewards for solo environment or info for Laserhockey\n",
    "                if env.unwrapped.spec is None:\n",
    "                    mean_steps.append(step)\n",
    "                    if info.get('winner') == 1: player1_won += 1\n",
    "                    elif info.get('winner') == -1: player2_won += 1\n",
    "                    else: no_one_won += 1; print('No one won.')\n",
    "                \n",
    "                all_rewards.append(np.sum(episode_rewards))    \n",
    "                print(\"Timestep: \", step)\n",
    "                print(\"Episode: \", episode)\n",
    "                \n",
    "                if env.unwrapped.spec is not None:\n",
    "                    print(\"Reward: \", all_rewards[-1])\n",
    "                    print(\"Max reward so far: \", np.max(all_rewards))\n",
    "                \n",
    "                \n",
    "                # Add reward and losses to Tensorboard\n",
    "                if not (IMITATION and WINNER) or winner != 0:\n",
    "                    new_action_labels = match_action_labels(action_labels)\n",
    "                    summary_ = sess.run(summary, feed_dict={input_: observations,\n",
    "                                                            action_label_: new_action_labels,\n",
    "                                                            advantage_: advantages,\n",
    "                                                            clipped_advantage_: clipped_advantages,\n",
    "                                                            reward_: all_rewards[-1],\n",
    "#                                                             actor_loss_: actor_loss.history['loss'][-1],\n",
    "                                                            critic_loss_: critic_loss.history['loss'][-1]})\n",
    "                    writer.add_summary(summary_, episode)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                print(\"=====================================\")\n",
    "                \n",
    "                break\n",
    "        \n",
    "        \n",
    "        # Save Model every 50 episodes\n",
    "        if episode % 50 == 0:\n",
    "            tf.train.Saver().save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")\n",
    "    tf.train.Saver().save(sess, \"./models/model.ckpt\")\n",
    "    env.close()\n",
    "    \n",
    "    if env.unwrapped.spec is None:\n",
    "        print('Player 1 won: ' + str(player1_won) + '/' + str(max_episodes))\n",
    "        print('Player 2 won: ' + str(player2_won) + '/' + str(max_episodes))\n",
    "        print('No one won: ' + str(no_one_won) + '/' + str(max_episodes))\n",
    "        print('Mean steps: ' + str(mean_list(mean_steps)))\n",
    "        print(mean_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T20:24:20.600553Z",
     "start_time": "2020-03-30T20:24:20.444662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "plt.plot(all_rewards)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Show agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T02:41:54.114321Z",
     "start_time": "2020-03-29T02:41:53.883485Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Controls:\n",
      " left:\t\t\tleft arrow key left\n",
      " right:\t\t\tarrow key right\n",
      " up:\t\t\tarrow key up\n",
      " down:\t\t\tarrow key down\n",
      " tilt clockwise:\td\n",
      " tilt anti-clockwise:\ta\n"
     ]
    }
   ],
   "source": [
    "human = lh.HumanOpponent(env=env, player=1)\n",
    "basic = lh.BasicOpponent()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T02:46:44.376065Z",
     "start_time": "2020-03-29T02:46:43.366777Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Timestep:  9\n",
      "Episode:  1\n",
      "Reward:  9.0\n",
      "========================================\n",
      "Timestep:  9\n",
      "Episode:  2\n",
      "Reward:  9.0\n",
      "========================================\n",
      "Timestep:  9\n",
      "Episode:  3\n",
      "Reward:  9.0\n",
      "========================================\n",
      "Timestep:  8\n",
      "Episode:  4\n",
      "Reward:  8.0\n",
      "========================================\n",
      "Timestep:  9\n",
      "Episode:  5\n",
      "Reward:  9.0\n",
      "========================================\n",
      "Timestep:  10\n",
      "Episode:  6\n",
      "Reward:  10.0\n",
      "========================================\n",
      "Timestep:  9\n",
      "Episode:  7\n",
      "Reward:  9.0\n",
      "========================================\n",
      "Timestep:  8\n",
      "Episode:  8\n",
      "Reward:  8.0\n",
      "========================================\n",
      "Timestep:  8\n",
      "Episode:  9\n",
      "Reward:  8.0\n",
      "========================================\n",
      "Timestep:  10\n",
      "Episode:  10\n",
      "Reward:  10.0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "if env.unwrapped.spec is None:\n",
    "    human = lh.HumanOpponent(env=env, player=1)\n",
    "    basic = lh.BasicOpponent()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    rewards, mean_steps = [], []\n",
    "    player1_won, player2_won, no_one_won = 0, 0, 0\n",
    "\n",
    "#     Load the model\n",
    "    tf.train.Saver().restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    max_test_episodes = 10\n",
    "    max_test_steps = 200\n",
    "    eps = 0\n",
    "    \n",
    "    for episode in range(1, max_test_episodes + 1):\n",
    "        total_rewards = 0\n",
    "\n",
    "        observation = env.reset()\n",
    "        if env.unwrapped.spec is None: obs_agent_2 = env.obs_agent_two()\n",
    "            \n",
    "        for step in range(1, max_test_steps + 1):\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            if random.random() < eps:\n",
    "                action = random.choice(actionspace)\n",
    "                action_2 = random.choice(actionspace)\n",
    "            else:\n",
    "                action_label = []\n",
    "                for i in range(policy_dim):\n",
    "                    action_vector = sess.run(action_distribution[i], feed_dict={input_: observation.reshape(1, -1)})[0]\n",
    "                    action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "#                 action = action_label_to_action(action_label)\n",
    "#                 action = human.act(observation)\n",
    "\n",
    "                # Same process for second player in LaserHockey\n",
    "                if env.unwrapped.spec is None:\n",
    "#                     action_label = []\n",
    "#                     for i in range(policy_dim):\n",
    "#                         action_vector = sess.run(action_distribution[i], feed_dict={input_: obs_agent_2.reshape(1, -1)})[0]\n",
    "#                         action_label.append(np.random.choice(action_dim, p=action_vector))\n",
    "#                     action_2 = action_label_to_action(action_label)\n",
    "            \n",
    "#                     action = basic.act(observation)\n",
    "                    action_2 = basic.act(obs_agent_2)\n",
    "#                     action = descrete_action(basic.act(observation))\n",
    "#                     action_2 = descrete_action(basic.act(obs_agent_2))\n",
    "            \n",
    "            # np.hstack for LaserHockey environment\n",
    "            if env.unwrapped.spec == None:\n",
    "                \n",
    "                observation, reward, done, info = env.step(np.hstack([action, action_2]))\n",
    "                obs_agent_2 = env.obs_agent_two()\n",
    "                \n",
    "            else: observation, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done or step == max_test_steps:\n",
    "                # Print reward for solo environment or info for Laserhockey\n",
    "                if env.unwrapped.spec is None:\n",
    "                    mean_steps.append(step)\n",
    "                    if info.get('winner') == 1: player1_won += 1\n",
    "                    elif info.get('winner') == -1: player2_won += 1\n",
    "                    else: no_one_won += 1; print('No one won.')\n",
    "                \n",
    "                print(\"Timestep: \", step)\n",
    "                print(\"Episode: \", episode)\n",
    "                \n",
    "                if env.unwrapped.spec is not None:\n",
    "                    rewards.append(total_rewards)\n",
    "                    print(\"Reward: \", total_rewards)\n",
    "                    \n",
    "                print(\"========================================\")\n",
    "                    \n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "    if env.unwrapped.spec is None:\n",
    "        print('Player 1 won: ' + str(player1_won) + '/' + str(max_test_episodes))\n",
    "        print('Player 2 won: ' + str(player2_won) + '/' + str(max_test_episodes))\n",
    "        print('No one won: ' + str(no_one_won) + '/' + str(max_test_episodes))\n",
    "        print('Mean steps: ' + str(mean_list(mean_steps)))\n",
    "        print(mean_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1203..1508 -> 305-tiles track\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\runlist.py\u001b[0m in \u001b[0;36mranges\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmin_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                     \u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-89910364cb49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CarRacing-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys_to_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\utils\\play.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(env, transpose, fps, zoom, callback, keys_to_action)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mIf\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mkey_to_action\u001b[0m \u001b[0mmapping\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0menv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprovided\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \"\"\"\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mrendered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    348\u001b[0m             self.score_label = pyglet.text.Label('0000', font_size=36,\n\u001b[0;32m    349\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWINDOW_H\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m40.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 color=(255,255,255,255))\n\u001b[0m\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, font_name, font_size, bold, italic, color, x, y, width, height, anchor_x, anchor_y, align, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    431\u001b[0m         super(Label, self).__init__(document, x, y, width, height, \n\u001b[0;32m    432\u001b[0m                                     \u001b[0manchor_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                                     multiline, dpi, batch, group)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         self.document.set_style(0, len(self.document.text), {\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, x, y, width, height, anchor_x, anchor_y, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    255\u001b[0m                                             \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                                             \u001b[0mmultiline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                             dpi=dpi, batch=batch, group=group)\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, width, height, multiline, dpi, batch, group, wrap_lines)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wrap_lines_invariant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_set_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_handlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     document = property(_get_document, _set_document,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_init_document\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_uninit_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mcolors_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_style_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0mlen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         \u001b[0mglyphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_glyphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m         \u001b[0mowner_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_owner_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner_runs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_glyphs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m             self._document.get_element_runs()))\n\u001b[0;32m   1011\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1012\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m                 \u001b[0mglyphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_InlineElementBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "from gym.utils import play\n",
    "\n",
    "\n",
    "key_mapping = {(97,108): 3, (100,108): 1, (108,119): 0, (108,115): 2}\n",
    "\n",
    "\n",
    "def callback(obs_t, obs_tp1, action, rew, done, info):\n",
    "    return [rew,]\n",
    "plotter = play.PlayPlot(callback, 30 * 1, [\"reward\"])\n",
    "\n",
    "\n",
    "env = gym.make('CarRacing-v0')\n",
    "play.play(env, callback=None, keys_to_action=key_mapping)\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1040..1312 -> 272-tiles track\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\runlist.py\u001b[0m in \u001b[0;36mranges\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmin_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                     \u001b[0mstarts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mends\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-03ffa551ce5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CarRacing-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state_pixels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    348\u001b[0m             self.score_label = pyglet.text.Label('0000', font_size=36,\n\u001b[0;32m    349\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWINDOW_H\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2.5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m40.00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 color=(255,255,255,255))\n\u001b[0m\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, font_name, font_size, bold, italic, color, x, y, width, height, anchor_x, anchor_y, align, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    431\u001b[0m         super(Label, self).__init__(document, x, y, width, height, \n\u001b[0;32m    432\u001b[0m                                     \u001b[0manchor_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                                     multiline, dpi, batch, group)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         self.document.set_style(0, len(self.document.text), {\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, x, y, width, height, anchor_x, anchor_y, multiline, dpi, batch, group)\u001b[0m\n\u001b[0;32m    255\u001b[0m                                             \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                                             \u001b[0mmultiline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                             dpi=dpi, batch=batch, group=group)\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, document, width, height, multiline, dpi, batch, group, wrap_lines)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wrap_lines_invariant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_set_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush_handlers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     document = property(_get_document, _set_document,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_init_document\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_uninit_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mcolors_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_style_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0mlen_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m         \u001b[0mglyphs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_glyphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m         \u001b[0mowner_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_owner_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mowner_runs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\OpenAI Gym\\lib\\site-packages\\pyglet\\text\\layout.py\u001b[0m in \u001b[0;36m_get_glyphs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m             self._document.get_element_runs()))\n\u001b[0;32m   1011\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_document\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1012\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m                 \u001b[0mglyphs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_InlineElementBox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CarRacing-v0')\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = {[1,2]:0}\n",
    "ord('w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "302px",
    "left": "1375px",
    "top": "133px",
    "width": "388px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
